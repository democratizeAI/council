#!/usr/bin/env python3
"""
AutoGen Titanic Gauntlet - Direct Testing
=========================================

Modified version of Titanic Gauntlet that tests AutoGen skills directly,
giving us real benchmark numbers without needing a web server.
"""

import asyncio
import json
import time
import random
from datetime import datetime
from pathlib import Path
import sys

# Add AutoGen skills
sys.path.append('fork/swarm_autogen')
from router_cascade import route_and_execute

# Import content guards for validation
from content_guards import strict_grader, ContentError

def generate_titanic_prompts(count: int = 50) -> list:
    """Generate a diverse set of prompts for benchmarking"""
    
    # Math prompts (30%)
    math_prompts = [
        "What is 12 factorial?",
        "Calculate 35% of 850",
        "Find the GCD of 48 and 72", 
        "What is the area of a circle with radius 15?",
        "Solve: 2x + 5 = 17",
        "What is 8^3?",
        "Calculate the square root of 144",
        "Find the LCM of 12 and 18",
        "What is 15% of 240?",
        "Calculate: (25 * 4) + (18 / 3)",
        "What is 7 factorial?",
        "Find the area of a triangle with base 10 and height 6",
        "Calculate 45% of 200",
        "What is 9^2?",
        "Find the perimeter of a rectangle 8x12"
    ]
    
    # Code prompts (25%)  
    code_prompts = [
        "Write a function to calculate GCD of two numbers",
        "Write a function to add two numbers",
        "Implement a bubble sort algorithm",
        "Write a function to check if a number is prime",
        "Create a function to reverse a string",
        "Write a binary search function", 
        "Implement a factorial function",
        "Write a function to find max in a list",
        "Create a palindrome checker function",
        "Write a function to count vowels in a string",
        "Implement quicksort algorithm",
        "Write a function to calculate Fibonacci sequence",
        "Create a function to merge two sorted arrays"
    ]
    
    # Logic prompts (20%)
    logic_prompts = [
        "If A is south of B and B south of C, where is A relative to C?",
        "Is John the parent of Mary if Mary is John's daughter?",
        "If all cats are mammals and Fluffy is a cat, is Fluffy a mammal?",
        "If some birds can fly and penguins are birds, can all birds fly?",
        "If all roses are flowers and some flowers are red, are all roses red?",
        "If Tom is taller than Sam and Sam is taller than Bill, who is shortest?",
        "If it's raining, then the ground is wet. The ground is dry. Is it raining?",
        "If all bloops are razzles and some razzles are lazzles, are all bloops lazzles?",
        "If Alice is older than Bob and Bob is older than Carol, who is youngest?",
        "If no fish are mammals and dolphins are mammals, are dolphins fish?"
    ]
    
    # Knowledge prompts (15%)
    knowledge_prompts = [
        "What is the speed of light?",
        "What is machine learning?", 
        "Explain how photosynthesis works",
        "What is DNA?",
        "Who wrote Romeo and Juliet?",
        "What is the capital of France?",
        "Explain quantum mechanics briefly",
        "What causes ocean tides?",
        "What is artificial intelligence?",
        "Explain how the internet works"
    ]
    
    # Creative prompts (10%)
    creative_prompts = [
        "Write a haiku about programming",
        "Create a short story about a robot",
        "Write a poem about the ocean",
        "Describe a futuristic city",
        "Write a dialogue between two AIs"
    ]
    
    # Combine all prompts
    all_prompts = []
    
    # Calculate counts for each category
    math_count = int(count * 0.30)
    code_count = int(count * 0.25) 
    logic_count = int(count * 0.20)
    knowledge_count = int(count * 0.15)
    creative_count = count - (math_count + code_count + logic_count + knowledge_count)
    
    # Sample from each category
    all_prompts.extend(random.sample(math_prompts, min(math_count, len(math_prompts))))
    all_prompts.extend(random.sample(code_prompts, min(code_count, len(code_prompts))))
    all_prompts.extend(random.sample(logic_prompts, min(logic_count, len(logic_prompts))))
    all_prompts.extend(random.sample(knowledge_prompts, min(knowledge_count, len(knowledge_prompts))))
    all_prompts.extend(random.sample(creative_prompts, min(creative_count, len(creative_prompts))))
    
    # Shuffle final list
    random.shuffle(all_prompts)
    
    return all_prompts

async def run_autogen_titanic_gauntlet(budget_usd: float = 10.0, target_prompts: int = 50):
    """Run the AutoGen Titanic Gauntlet"""
    
    print("ğŸš¢ AUTOGEN TITANIC GAUNTLET")
    print("=" * 60)
    start_time = datetime.now()
    print(f"â° Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ’° Budget limit: ${budget_usd}")
    print(f"ğŸ“Š Target prompts: {target_prompts}")
    print()
    
    # Generate prompts
    prompts = generate_titanic_prompts(target_prompts)
    print(f"ğŸ“ Generated {len(prompts)} prompts for testing")
    print()
    
    # Results tracking
    results = []
    total_cost = 0.0  # We'll track estimated cost
    successes = 0
    errors = 0
    total_latency = 0.0
    
    # Content validation tracking
    content_passes = 0
    content_fails = 0
    
    for i, prompt in enumerate(prompts, 1):
        print(f"ğŸ“ Test {i}/{len(prompts)}: {prompt[:50]}...")
        
        # Budget check
        if total_cost >= budget_usd:
            print(f"ğŸ’° Budget limit reached: ${total_cost:.2f} >= ${budget_usd}")
            break
        
        start_request = time.time()
        
        try:
            # Execute with AutoGen
            result = await route_and_execute(prompt)
            
            latency = (time.time() - start_request) * 1000
            total_latency += latency
            
            # Extract response
            answer = result.get("answer", "No answer")
            skill_type = result.get("skill_type", "unknown")
            confidence = result.get("confidence", 0.0)
            
            # Estimate cost (mock for AutoGen - actual models are local)
            estimated_cost = 0.001  # $0.001 per request for local inference
            total_cost += estimated_cost
            
            # Content validation
            content_valid = True
            content_details = "Not validated"
            
            try:
                # Basic content validation based on skill type
                if skill_type == "math" and any(char.isdigit() for char in answer):
                    content_passes += 1
                    content_details = "Math answer contains numbers"
                elif skill_type == "code" and ("def " in answer or "function" in answer):
                    content_passes += 1
                    content_details = "Code contains function definition"
                elif skill_type == "logic" and any(word in answer.lower() for word in ["yes", "no", "true", "false"]):
                    content_passes += 1
                    content_details = "Logic answer contains decision"
                elif skill_type == "knowledge" and len(answer) > 10:
                    content_passes += 1
                    content_details = "Knowledge answer has substance"
                else:
                    content_fails += 1
                    content_details = "Content validation failed"
                    content_valid = False
                
            except Exception as e:
                content_fails += 1
                content_details = f"Validation error: {e}"
                content_valid = False
            
            print(f"    âš¡ {latency:5.0f}ms | ğŸ¯ {skill_type} | ğŸ“Š {confidence:.2f} | ğŸ’° ${estimated_cost:.3f}")
            print(f"    ğŸ’¬ {answer[:60]}...")
            print(f"    âœ… Content: {content_details}")
            
            successes += 1
            
            results.append({
                "prompt": prompt,
                "answer": answer,
                "skill_type": skill_type,
                "confidence": confidence,
                "latency_ms": latency,
                "cost_usd": estimated_cost,
                "content_valid": content_valid,
                "content_details": content_details,
                "status": "success"
            })
            
        except Exception as e:
            latency = (time.time() - start_request) * 1000
            total_latency += latency
            errors += 1
            
            print(f"    âŒ Error: {str(e)[:50]}...")
            print(f"    âš¡ {latency:5.0f}ms | ğŸ¯ error")
            
            results.append({
                "prompt": prompt,
                "answer": f"Error: {e}",
                "skill_type": "error",
                "confidence": 0.0,
                "latency_ms": latency,
                "cost_usd": 0.0,
                "content_valid": False,
                "content_details": "Execution error",
                "status": "error"
            })
        
        print()
        
        # Small delay between requests
        await asyncio.sleep(0.1)
    
    # Final statistics
    end_time = datetime.now()
    total_time = (end_time - start_time).total_seconds()
    completed_requests = len(results)
    avg_latency = total_latency / completed_requests if completed_requests > 0 else 0
    success_rate = successes / completed_requests if completed_requests > 0 else 0
    content_pass_rate = content_passes / (content_passes + content_fails) if (content_passes + content_fails) > 0 else 0
    
    print("ğŸ AUTOGEN TITANIC GAUNTLET COMPLETE!")
    print("=" * 60)
    print(f"â° Duration: {total_time:.1f}s")
    print(f"ğŸ“Š Requests completed: {completed_requests}/{target_prompts}")
    print(f"âœ… Success rate: {success_rate*100:.1f}% ({successes}/{completed_requests})")
    print(f"âš¡ Average latency: {avg_latency:.0f}ms")
    print(f"ğŸš€ Throughput: {completed_requests/total_time:.1f} QPS")
    print(f"ğŸ’° Total cost: ${total_cost:.3f}")
    print(f"ğŸ“‹ Content validation: {content_pass_rate*100:.1f}% ({content_passes}/{content_passes + content_fails})")
    
    # Performance targets check
    print(f"\nğŸ¯ PERFORMANCE TARGETS:")
    latency_ok = avg_latency < 400
    content_ok = content_pass_rate >= 0.80
    print(f"    Latency < 400ms: {'âœ…' if latency_ok else 'âŒ'} ({avg_latency:.0f}ms)")
    print(f"    Content accuracy â‰¥ 80%: {'âœ…' if content_ok else 'âŒ'} ({content_pass_rate*100:.1f}%)")
    
    if latency_ok and content_ok:
        print("âœ… ALL TARGETS MET - Ready for production!")
    else:
        print("âŒ Some targets missed - needs optimization")
    
    # Save detailed report
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = f"reports/autogen_titanic_{timestamp}.json"
    
    Path("reports").mkdir(exist_ok=True)
    
    final_report = {
        "metadata": {
            "timestamp": end_time.isoformat(),
            "duration_seconds": total_time,
            "budget_usd": budget_usd,
            "target_prompts": target_prompts,
            "completed_requests": completed_requests
        },
        "performance": {
            "success_rate": success_rate,
            "avg_latency_ms": avg_latency,
            "throughput_qps": completed_requests/total_time,
            "total_cost_usd": total_cost,
            "content_pass_rate": content_pass_rate
        },
        "targets": {
            "latency_target_met": latency_ok,
            "content_target_met": content_ok,
            "all_targets_met": latency_ok and content_ok
        },
        "detailed_results": results
    }
    
    with open(report_file, 'w') as f:
        json.dump(final_report, f, indent=2)
    
    print(f"\nğŸ“„ Detailed report saved: {report_file}")
    
    return final_report

if __name__ == "__main__":
    import sys
    
    # Parse command line arguments
    budget = float(sys.argv[1]) if len(sys.argv) > 1 else 10.0
    prompts = int(sys.argv[2]) if len(sys.argv) > 2 else 50
    
    # Run the gauntlet
    asyncio.run(run_autogen_titanic_gauntlet(budget, prompts)) 