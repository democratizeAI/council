version: '3.8'

services:
  # Model Downloader (runs once to fetch models)
  model-downloader:
    image: alpine/git:latest
    container_name: autogen-model-downloader
    volumes:
      - ./models:/models
    command: >
      sh -c "
        if [ ! -d '/models/mistral-13b-gptq' ]; then
          echo 'ğŸ“¥ Downloading model (this may take 10-15 minutes)...'
          apk add --no-cache curl
          mkdir -p /models/mistral-13b-gptq
          # Download model files from HuggingFace
          cd /models/mistral-13b-gptq
          curl -L -o config.json https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ/raw/main/config.json
          curl -L -o tokenizer.json https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ/raw/main/tokenizer.json
          curl -L -o tokenizer_config.json https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GPTQ/raw/main/tokenizer_config.json
          echo 'âœ… Model download complete'
        else
          echo 'âœ… Model already exists'
        fi
      "
    restart: "no"

  # ExLlamaV2 LLM Backend (optional for GPU setups)
  llm:
    image: ghcr.io/turboderp/exllamav2:latest
    container_name: autogen-llm
    ports:
      - "8001:8000"
    volumes:
      - ./models:/models
    command: [
      "--model", "/models/mistral-13b-gptq",
      "--listen", "0.0.0.0:8000", 
      "--dtype", "auto",
      "--max-input-tokens", "4096"
    ]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=0
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    profiles:
      - gpu
      - full

  # AutoGen Council v2.6.0 - Memory-Powered Desktop OS Assistant
  council:
    build: .
    container_name: autogen-council
    ports:
      - "8000:8000"
    volumes:
      - memory-store:/app/memory_store
      - ./logs:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - PROMETHEUS_MULTIPROC_DIR=/tmp/prometheus
      - AZ_MEMORY_ENABLED=yes
      - AZ_MEMORY_PATH=/app/memory_store
      - AZ_SHELL_TRUSTED=yes
      - ENABLE_SANDBOX=true
      - CLOUD_ENABLED=true
      - PROVIDER_PRIORITY=mistral,openai
      - SWARM_PROFILE=${SWARM_PROFILE:-production}
      - MEMORY_DIMENSION=${MEMORY_DIMENSION:-384}
      - SWARM_MAX_CONCURRENT=10
      - SWARM_TIMEOUT_MS=5000
      - SWARM_CLOUD_BUDGET_USD=10.0
      - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    cap_add:
      - SYS_ADMIN
    security_opt:
      - seccomp:unconfined
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # System Health Checker - v2.6.0 Enhanced
  health-checker:
    image: curlimages/curl:latest
    container_name: autogen-health-checker
    depends_on:
      council:
        condition: service_healthy
    command: >
      sh -c "
        echo 'ğŸ” Running AutoGen Council v2.6.0 health check...'
        sleep 10
        
        # Test Council API
        echo 'ğŸ“¡ Testing Council API...'
        if curl -f http://council:8000/health; then
          echo 'âœ… Council API is healthy'
        else
          echo 'âŒ Council API failed health check'
          exit 1
        fi
        
        # Test v2.6.0 capabilities
        echo 'ğŸ§ª Testing v2.6.0 capabilities...'
        
        # Test memory system
        echo '  ğŸ§  Testing memory system...'
        response=$$(curl -s -X POST http://council:8000/hybrid \\
          -H 'Content-Type: application/json' \\
          -d '{\"prompt\":\"Remember my name is TestUser\"}')
        echo \"  Memory test response: $$response\"
        
        # Test math specialist
        echo '  ğŸ§® Testing math specialist...'
        response=$$(curl -s -X POST http://council:8000/hybrid \\
          -H 'Content-Type: application/json' \\
          -d '{\"prompt\":\"What is 2+2?\"}')
        
        if echo \"$$response\" | grep -q '4'; then
          echo '  âœ… Math specialist working!'
        else
          echo '  âš ï¸ Math specialist response: '$$response
        fi
        
        # Test sandbox execution (if enabled)
        echo '  ğŸ›¡ï¸ Testing sandbox execution...'
        response=$$(curl -s -X POST http://council:8000/hybrid \\
          -H 'Content-Type: application/json' \\
          -d '{\"prompt\":\"Run this Python code: print(\\\"Hello from sandbox\\\")\"}')
        echo \"  Sandbox test response: $$response\"
        
        echo ''
        echo 'ğŸ‰ AutoGen Council v2.6.0 is ready!'
        echo '=================================='
        echo 'ğŸŒ Main API: http://localhost:8000'
        echo 'ğŸ“Š Health: http://localhost:8000/health'
        echo 'ğŸ“ˆ Stats: http://localhost:8000/stats'
        echo 'ğŸ“Š Metrics: http://localhost:8000/metrics'
        echo 'ğŸ“ˆ Grafana: http://localhost:3000 (admin/autogen123)'
        echo 'ğŸ“Š Prometheus: http://localhost:9090'
        echo ''
        echo 'âœ… Memory Persistence: FAISS vector storage enabled'
        echo 'âœ… Secure Code Execution: Firejail sandbox enabled'
        echo 'âœ… 4 Specialist Skills: Math, Code, Logic, Knowledge'
        echo 'âœ… Production Monitoring: Prometheus + Grafana'
      "
    restart: "no"
    profiles:
      - test
      - full

  # Prometheus monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=7d'
    restart: unless-stopped
    profiles:
      - monitoring
      - full

  # Grafana dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=autogen123
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning:ro
    restart: unless-stopped
    depends_on:
      - prometheus
    profiles:
      - monitoring
      - full

  # AlertManager for handling alerts
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
      - '--web.listen-address=0.0.0.0:9093'
    restart: unless-stopped
    profiles:
      - monitoring
      - full

volumes:
  grafana-storage:
  prometheus-data:
  alertmanager-data:
  memory-store:
    driver: local
  
# Networks for better isolation
networks:
  default:
    name: autogen-network 