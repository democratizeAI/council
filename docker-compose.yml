version: '3.8'

services:
  # Redis for LLM call caching and Agent-0 state
  redis:
    image: redis:7-alpine
    container_name: council-redis
    ports:
      - "6379:6379"
    restart: on-failure:3
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "1.0"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 3s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Main Council API - Stable traffic
  council-api:
    build: .
    container_name: council-api
    ports:
      - "8000:8000"
    volumes:
      - models:/app/models
      - memory:/app/memory
      - loras:/app/loras
      - ./logs:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - AZ_MEMORY_ENABLED=yes
      - AZ_MEMORY_PATH=/app/memory
      - AZ_SHELL_TRUSTED=yes
      - ENABLE_SANDBOX=true
      - CLOUD_ENABLED=true
      - PROVIDER_PRIORITY=mistral,openai
      - SWARM_PROFILE=production
      - SWARM_CLOUD_BUDGET_USD=10.0
      - MEM_EMB_MODEL=all-MiniLM-L6-v2
      - EXEC_TIMEOUT_SEC=5
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
      - COUNCIL_TRAFFIC_PERCENT=95
      - REDIS_URL=redis://redis:6379
      - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 6g     # Leave 2-3GB headroom for OS + other containers
          cpus: "6.0"
        reservations:
          devices:
            - capabilities: ["gpu"]
    depends_on:
      - redis
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "curl", "-fs", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Canary Council API - New LoRA testing
  council-api-canary:
    build: .
    container_name: council-api-canary
    ports:
      - "8001:8000"
    volumes:
      - models:/app/models
      - memory:/app/memory
      - loras:/app/loras
      - ./logs:/app/logs
    environment:
      - PYTHONUNBUFFERED=1
      - AZ_MEMORY_ENABLED=yes
      - AZ_MEMORY_PATH=/app/memory
      - AZ_SHELL_TRUSTED=yes
      - ENABLE_SANDBOX=true
      - CLOUD_ENABLED=true
      - PROVIDER_PRIORITY=mistral,openai
      - SWARM_PROFILE=canary
      - SWARM_CLOUD_BUDGET_USD=1.5
      - MEM_EMB_MODEL=all-MiniLM-L6-v2
      - EXEC_TIMEOUT_SEC=5
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
      - COUNCIL_TRAFFIC_PERCENT=5
      - REDIS_URL=redis://redis:6379
      - LORA_PATH=/app/loras/latest
      - MISTRAL_API_KEY=${MISTRAL_API_KEY:-}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 3g     # Smaller allocation for canary
          cpus: "4.0"
        reservations:
          devices:
            - capabilities: ["gpu"]
    depends_on:
      - redis
      - council-api
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "curl", "-fs", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Agent Zero Runner for failure rewriting
  agent0-runner:
    build:
      context: .
      dockerfile: Dockerfile.agent0
    container_name: agent0-runner
    volumes:
      - memory:/app/memory
      - workspace:/app/workspace
    environment:
      - PYTHONUNBUFFERED=1
      - COUNCIL_API_URL=http://council-api:8000
    depends_on:
      - council-api
    restart: unless-stopped

  # Harvest Job - Nightly failure collection and rewriting
  harvest-job:
    image: python:3.11-slim
    container_name: harvest-job
    volumes:
      - memory:/app/memory
      - data-lora:/app/data/lora
      - ./tools:/app/tools
      - ./scripts:/app/scripts
    environment:
      - PYTHONUNBUFFERED=1
      - AGENT0_URL=http://agent0-runner:8080
      - REDIS_URL=redis://redis:6379
    command: >
      sh -c "
        apt-get update && apt-get install -y cron curl python3-pip &&
        pip install requests redis faiss-cpu &&
        echo '0 2 * * * cd /app && python /app/tools/harvest_failures.py' | crontab - &&
        cron -f
      "
    depends_on:
      - agent0-runner
      - redis
    restart: unless-stopped

  # LoRA Trainer - Nightly fine-tuning
  lora-trainer:
    build:
      context: .
      dockerfile: Dockerfile.trainer
    container_name: lora-trainer
    volumes:
      - loras:/app/loras
      - data-lora:/app/data/lora
      - models:/app/models
    environment:
      - PYTHONUNBUFFERED=1
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
      - TRAINING_BUDGET_USD=0.20
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    command: >
      sh -c "
        echo '0 3 * * * cd /app && python train_lora.py --data /app/data/lora/latest.jsonl --output /app/loras/latest' | crontab - &&
        cron -f
      "
    depends_on:
      - harvest-job
    restart: unless-stopped

  # Traefik Load Balancer with Canary Routing
  lb-traefik:
    image: traefik:v3.0
    container_name: traefik-lb
    ports:
      - "80:80"
      - "8080:8080"  # Traefik dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik:/etc/traefik
    command:
      - --api.insecure=true
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:80
      - --metrics.prometheus=true
    depends_on:
      - council-api
      - council-api-canary
    restart: unless-stopped
    labels:
      - "traefik.enable=true"

  # Static Web Interface
  static-web:
    image: nginx:alpine
    container_name: static-web
    ports:
      - "9000:80"
    volumes:
      - ./webchat:/usr/share/nginx/html/chat
      - ./admin.html:/usr/share/nginx/html/admin.html
      - ./index.html:/usr/share/nginx/html/index.html
    depends_on:
      - council-api
    restart: unless-stopped

  # Prometheus monitoring with enhanced config
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/alerts_simple.yml:/etc/prometheus/alerts.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=7d'
      - '--web.enable-admin-api'
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "2.0"
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Grafana dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=autogen123
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning:ro
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "1.0"
    restart: on-failure:3
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD", "curl", "-fs", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # AlertManager for handling alerts
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
      - '--web.listen-address=0.0.0.0:9093'
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.5"
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

volumes:
  models:        # Static GGUF/Q4_K_M files
  loras:         # Nightly adapter outputs  
  memory:        # FAISS index & meta.jsonl
  workspace:     # Agent-0 file sandbox
  data-lora:     # Training data JSONLs
  prometheus-data:  # TSDB
  grafana-storage:
  alertmanager-data:

networks:
  default:
    name: council-network 