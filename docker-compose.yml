version: '3.8'

services:
  # Redis for LLM call caching and Agent-0 state
  redis:
    image: redis:7-alpine
    container_name: council-redis
    ports:
      - "6379:6379"
    restart: on-failure:3
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "1.0"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 3s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # Redis Exporter for Prometheus metrics
  redis_exporter:
    image: oliver006/redis_exporter:latest
    container_name: redis-exporter
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis://council-redis:6379
    depends_on:
      - redis
    restart: on-failure:3
    networks:
      - monitoring
    deploy:
      resources:
        limits:
          memory: 64m
          cpus: "0.1"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "2"

  # Main Council API - Stable traffic
  council-api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: council-api
    ports:
      - "9010:9000"
    volumes:
      - ./api:/app/api
      - ./router:/app/router
      - ./shared:/app/shared
      - ./ledger:/app/ledger
    environment:
      - PYTHONUNBUFFERED=1
      - COUNCIL_API_URL=http://council-api:9000
      - SLACK_WEBHOOK_ALERTS=${SLACK_WEBHOOK_ALERTS}
      - SLACK_WEBHOOK_PATCHES=${SLACK_WEBHOOK_PATCHES}
      - BUILDER_WEBHOOK_URL=http://builder:8080/webhook
      - GATEKEEPER_URL=http://gatekeeper:8080
      - REDIS_URL=redis://council-redis:6379
    command: >
      sh -c "
        cd /app/api &&
        python -m uvicorn council_api:app --host 0.0.0.0 --port 9000 --reload
      "
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - monitoring
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.council-api.rule=Host(`api.trinity.local`) || PathPrefix(`/api`)"
      - "traefik.http.services.council-api.loadbalancer.server.port=9000"

  # Canary Council API - New LoRA testing
  council-api-canary:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: council-api-canary
    ports:
      - "9011:9000"
    volumes:
      - ./api:/app/api
      - ./router:/app/router
      - ./shared:/app/shared
      - ./ledger:/app/ledger
    environment:
      - PYTHONUNBUFFERED=1
      - COUNCIL_API_URL=http://council-api-canary:9000
      - SLACK_WEBHOOK_ALERTS=${SLACK_WEBHOOK_ALERTS}
      - SLACK_WEBHOOK_PATCHES=${SLACK_WEBHOOK_PATCHES}
      - BUILDER_WEBHOOK_URL=http://builder:8080/webhook
      - GATEKEEPER_URL=http://gatekeeper:8080
      - REDIS_URL=redis://council-redis:6379
      - DEPLOYMENT_TYPE=canary
    command: >
      sh -c "
        cd /app/api &&
        python -m uvicorn council_api:app --host 0.0.0.0 --port 9000 --reload
      "
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - monitoring
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.council-api-canary.rule=(Host(`api.trinity.local`) || PathPrefix(`/api`)) && HeadersRegexp(`X-Canary`, `true`)"
      - "traefik.http.services.council-api-canary.loadbalancer.server.port=9000"

  # Slack Socket Mode Service
  slack-socket:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: slack-socket
    volumes:
      - ./services:/app/services
      - ./api:/app/api
    environment:
      - PYTHONUNBUFFERED=1
      - SLACK_APP_TOKEN=${SLACK_APP_TOKEN}
      - SLACK_BOT_TOKEN=${SLACK_BOT_TOKEN}
      - COUNCIL_API_URL=http://council-api:9000
      - SLACK_WEBHOOK_ALERTS=${SLACK_WEBHOOK_ALERTS}
      - SLACK_WEBHOOK_PATCHES=${SLACK_WEBHOOK_PATCHES}
    command: >
      sh -c "
        cd /app/services &&
        python slack_socket.py
      "
    depends_on:
      - council-api
    restart: unless-stopped
    networks:
      - monitoring
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Alert Slack Relay - Routes internal webhooks to Slack
  alert-slack:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: alert-slack
    ports:
      - "9005:9005"
    volumes:
      - ./services:/app/services
    environment:
      - PYTHONUNBUFFERED=1
      - SLACK_BOT_TOKEN=${SLACK_BOT_TOKEN}
      - SLACK_WEBHOOK_ALERTS=${SLACK_WEBHOOK_ALERTS}
      - SLACK_WEBHOOK_PATCHES=${SLACK_WEBHOOK_PATCHES}
    command: >
      sh -c "
        cd /app/services &&
        python alert_slack_relay.py
      "
    restart: unless-stopped
    networks:
      - monitoring
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9005/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.alert-slack.rule=PathPrefix(`/webhook`)"
      - "traefik.http.services.alert-slack.loadbalancer.server.port=9005"

  # Traefik Load Balancer with Canary Routing
  lb-traefik:
    image: traefik:v3.0
    container_name: traefik-lb
    ports:
      - "80:80"
      - "8080:8080"  # Traefik dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./traefik:/etc/traefik
    command:
      - --api.insecure=true
      - --providers.docker=true
      - --providers.docker.exposedbydefault=false
      - --entrypoints.web.address=:80
      - --metrics.prometheus=true
    depends_on:
      - council-api
      - council-api-canary
    restart: unless-stopped
    networks:
      - monitoring
    labels:
      - "traefik.enable=true"

  # Static Web Interface
  static-web:
    image: nginx:alpine
    container_name: static-web
    ports:
      - "9000:80"
    volumes:
      - ./webchat:/usr/share/nginx/html/chat
      - ./admin.html:/usr/share/nginx/html/admin.html
      - ./index.html:/usr/share/nginx/html/index.html
    depends_on:
      - council-api
    restart: unless-stopped
    networks:
      - monitoring

  # Prometheus monitoring with enhanced config
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./rules:/etc/prometheus/rules
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - monitoring

  # Grafana dashboard
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    networks:
      - monitoring

  # AlertManager for handling alerts
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
      - '--web.listen-address=0.0.0.0:9093'
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.5"
    restart: on-failure:3
    networks:
      - monitoring
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"

  # VRAM metrics exporter (hybrid implementation - metrics only)
  scheduler:
    build: ./scheduler
    container_name: scheduler
    ports:
      - "9108:8000"
    restart: unless-stopped
    networks:
      - monitoring
    environment:
      - PROMETHEUS_PORT=8000
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Trainer container (managed by host systemd timer)
  trainer:
    image: trainer:latest
    container_name: trainer
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    networks:
      - monitoring
    volumes:
      - ./models:/models
      - ./data:/data

  # Node exporter for textfile collector
  node_exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /var/lib/node_exporter/textfile_collector:/var/lib/node_exporter/textfile_collector:ro
    restart: unless-stopped
    networks:
      - monitoring

  # Patchctl microservice for automated patching
  patchctl:
    build: ./patchctl
    container_name: patchctl
    ports:
      - "9001:9000"
    volumes:
      - ./:/app
      - ./config:/config
    restart: unless-stopped
    networks:
      - monitoring

volumes:
  models:        # Static GGUF/Q4_K_M files
  loras:         # Nightly adapter outputs  
  memory:        # FAISS index & meta.jsonl
  workspace:     # Agent-0 file sandbox
  data-lora:     # Training data JSONLs
  prometheus-data:  # TSDB
  grafana-storage:
  alertmanager-data:

networks:
  monitoring:
    name: monitoring
    driver: bridge 