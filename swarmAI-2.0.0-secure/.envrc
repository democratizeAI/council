# SwarmAI environment configuration
# Use with direnv: https://direnv.net/

# GPU Profile (determines VRAM limits and model loading strategy)
export SWARM_GPU_PROFILE=rtx_4070

# CUDA memory allocator settings (prevents fragmentation at high QPS)
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# Model storage root (optional - customize for your setup)
# export SWARM_MODEL_ROOT=/path/to/your/models

# FastAPI development settings
export SWARM_DEBUG=true
export SWARM_LOG_LEVEL=INFO

# Locust load testing target (for CI vs local vs remote testing)
export SWARM_ROUTER_HOST=http://127.0.0.1:8000

# Prometheus metrics collection
export SWARM_METRICS_ENABLED=true

# CI-specific overrides
if [[ "${CI}" == "true" ]]; then
    export SWARM_GPU_PROFILE=gtx_1080  # Smaller memory footprint for CI
    export SWARM_DEBUG=false
    export SWARM_LOG_LEVEL=WARNING
fi

echo "ðŸ”§ SwarmAI environment loaded:"
echo "  GPU Profile: $SWARM_GPU_PROFILE"
echo "  CUDA Allocator: $PYTORCH_CUDA_ALLOC_CONF"
echo "  Router Host: $SWARM_ROUTER_HOST" 