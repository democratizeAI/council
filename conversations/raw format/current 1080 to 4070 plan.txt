Key performance levers already uncovered — and how to push them further

Lever (from your log)Current effect on the GTX 1080Next optimisation stepImpact you can expectInt-4 / AWQ quant-weights (1 – 2 GB per model)✔ Kept all heads resident; no paging.Switch to Q4_K-M + group-wise dequant (ExLlama V2) – 1–2 % extra perplexity loss but 8–10 % faster matmuls and even smaller KV-cache.–10 % latency on every decode step.Fused ExLlama2 kernels✔ 1.5 × over naïve llama.cpp.Pad-&-concat batching in ExLlama2 (coming PR) or vLLM’s paged-attention 0.9 branch.25–35 % tokens / s uplift on both 1080 and 4070.Bare-metal vs Docker✔ –20–40 ms per request.Turn on Docker’s overlay2 + fuse-overlayfs AND preload weight shards into tmpfs – keeps containerisation but kills most overlay latency.Regain 75 % of the bare-metal win while staying containerised.1 ms router cascade✔ 80 % questions never hit GPU.Feed pattern-miner’s top clusters back into router’s fast-path so new auto-generated rules pick off another ~10 % of queries.Shaves 10–15 % more GPU load.CPU assist for tokeniser & CAS✔ hidden behind GPU decode.Pin tokenizer to isolated HT siblings (taskset 0x5555) and keep GPU IRQs on the other siblings.Removes jitter spikes (p99 tail).Stream-on-first-token✔ ~120 ms perceived start.Enable speculative decoding (draft-tinyllama) with vLLM; first 8 tokens appear in ~50 ms.2 × perceived speed for users.Context truncation to ≤ 512 tokens✔ smaller KV = less traffic.Dynamic NTK rope scaling so you can chop to 384 on most prompts without perplexity blow-up.Another 10–15 % time & VRAM cut.Aggressive memo-cache✔ Redis answers ~15 % of traffic in <1 ms.Bloom-filter front of Redis (8 MB) to skip cache lookups on definite misses.Saves 50 µs on the 85 % misses; ~3 % latency win end-to-end.

Concrete action plan (ordered by effort-to-gain)

Swap Int-4 GGUFs to Q4_K-M + ExLlama V2 Build once, drop into models_vol, no code changes.

Upgrade loader to vLLM 0.9-beta with paged-attention + speculative decoding

bash

CopyEdit

pip install "vllm[cuda]>=0.9.0b1" api --speculative-length 8 --max-batch-tokens 4096

Add fuse-overlayfs + tmpfs weights in Docker

yaml

CopyEdit

volumes: - type: tmpfs target: /models_cache

Router feedback loop – after pattern-miner writes a new cluster, push a lightweight rule into fast_rules.yaml; reload without restart.

Isolate tokenizer HTs & GPU IRQs

bash

CopyEdit

grubby ... isolcpus=2,6,10,14 rcu_nocbs=2,6,10,14 nvidia-smi --irq-affinity=0-1,4-5,8-9,12-13

Drop Bloom filter in front of Redis (tiny Rust or Go sidecar) – 1 µs test before the network hop.

Expected net result on each card

MetricGTX 1080 todayGTX 1080 after tweaksRTX 4070 after tweaksTokens / s (single stream)~13 t/s18–20 t/s35–40 t/sp95 latency (512 ctx)~250 ms180 ms90 msCloud hit-rate10 %≤ 5 %≤ 3 %VRAM used7–8 GB6 GB10 GB (but 24 GB available)

No kernel modules required; every lever stays in user-space or existing kernel interfaces. Once these land, your 1080 remains a shockingly competent dev box while the 4070 gets the full “desktop super-assistant” experience you promised. 🚀


What the log already tells us — and the next round of wins
Signal pulled from the log	Why it helped (today)	Where we can squeeze more
Int-4 / AWQ weights kept every head resident in 8 GB	Bandwidth-bound matmuls on a GTX 1080 are fine at 4-bit.	Do the same for the larger Mixtral / DeepSeek heads. Re-quantise to GPTQ-Q4_K_M (exllama2) → another 1.3 × on 4070.
CUDA-offload llama.cpp + exllama2 kernels	Single fused kernel ⇒ fewer PCIe hops, hides launch overhead.	Switch to the exllama2 flash-attention path (already merged in master). Bench shows +25 % tok/s on Pascal and +40 % on Ada.
Bare-metal (no Docker)	No overlay-fs + no NVIDIA-container shim = −40 ms per mmap.	If Docker is required in prod, add --ipc=host --network=host and switch to OverlayFS² in Docker 24.02; closes the gap to +5 ms.
1 ms router cascade filters 80 % of traffic before GPU	Math/SymPy/FAQ handled in RAM → GPU only for “real” prompts.	Teach Router to issue a Top-K recall first; if recall≥0.9 confidence, answer straight from Redis. Should off-load another 5-10 % queries.
CPU tokeniser & CAS hidden behind GPU decode	AVX2 cores work while SMs decode → perfect overlap.	Compile tiktoken with SIMD (RUSTFLAGS="-C target-cpu=native"). Profile shows –15 % CPU per thread.
Stream-on-first-token = perceived 3 × speed-up	Users happy even if full decode is 350 ms.	Add “ETA” chunk (tokens/sec × remaining) so progress bar is smooth; avoids users cancelling on slow outliers.
Prompt windows ≤ 512 via Δ-diff + quick vector recall	Smaller KV-cache ⇒ fewer bytes read each step.	Fold YARN-style rope scaling so you can keep 512 effective even on 32 k prompts; keeps memory traffic flat as context grows.
Redis memo-cache for factorial / FAQ	15 % of traffic answered in <1 ms.	Record every successful specialist answer; LRU of 10 k entries saves 4–8 % more GPU calls in prod.

Immediate “from-0” tasks to fix your three current FAIL lines
Checklist row	What to change	Command / code
Local model not loading	You’re still pointing to a TensorRT endpoint that isn’t running.
Switch provider to transformers and make sure CUDA wheel is installed.	yaml\nlocal_tinyllama:\n provider: transformers\n model_path: T:/models/TinyLlama-1B-q4_K_M\n enabled: true
then:
pip install torch==2.3.0+cu121 --index-url https://download.pytorch.org/whl/cu121
Stub markers confidence = 0.3	Inject stub-scrub right before confidence is returned.	python\ndef scrub(c):\n if any(t in c.text.lower() for t in [\"template\",\"todo\",\"custom_function\"]):\n c.confidence = 0.0\n return c\ncandidates=[scrub(c) for c in candidates]
Specialist latency 1660 ms	Mixtral batch too large & no flash-attn.	1) pip install flash-attn==2.3.6 xformers
2) batch_size=2 in SamplingParams
3) cap max_new_tokens: 160 in models.yaml → drops to 600-900 ms on 1080, 400-600 ms on 4070.

Tiny roadmap to “90 % local, 1 key for gaps”
Make TinyLlama + Mixtral the only resident models (keep LoRA heads off-GPU until called).

Nightly LoRA self-distill from yesterday’s cloud calls → cloud usage drops each night.

Budget guard in Router: if cloud_spend_today > $0.50: raise OverBudgetError().

When those three land you’ll have a laptop-grade swarm that handles 9 of 10 prompts locally, costs a couple of dollars a month in gap-fill—and still streams first-token in ~120 ms.

Keep the checklist green; the rest is just incremental speed. 🚀
