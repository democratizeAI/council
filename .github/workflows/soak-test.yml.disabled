name: "FastAPI Soak Test (Ticket #217)"

on:
  workflow_dispatch:
    inputs:
      duration:
        description: 'Test duration (e.g., 5m, 10m)'
        required: false
        default: '5m'
      users:
        description: 'Number of users'
        required: false
        default: '150'
      spawn_rate:
        description: 'User spawn rate per second'
        required: false
        default: '25'
  
  # Auto-trigger on FastAPI changes
  push:
    paths:
      - 'api/**'
      - 'docker-compose.fastapi.yml'
      - 'monitoring/**'
  
  # Schedule for nightly soak tests
  schedule:
    - cron: '0 2 * * *'  # 2 AM daily

env:
  PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128"
  PROMETHEUS_ENABLED: "true"

jobs:
  soak-test:
    name: "ğŸ”¥ 5-Minute Soak Test"
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: "ğŸ“¥ Checkout code"
        uses: actions/checkout@v4
        
      - name: "ğŸ Setup Python"
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: "ğŸ“¦ Install dependencies"
        run: |
          pip install -r tests/requirements.txt
          pip install locust docker requests
          
      - name: "ğŸ‹ Setup Docker Buildx"
        uses: docker/setup-buildx-action@v3
        
      - name: "ğŸš€ Start FastAPI Stack"
        run: |
          echo "ğŸš€ Starting FastAPI stack for soak testing..."
          docker-compose -f docker-compose.fastapi.yml up -d --build
          
          echo "â³ Waiting for services to become healthy..."
          sleep 30
          
          # Wait for main API
          timeout 60 bash -c 'until curl -f http://localhost:8000/healthz; do sleep 2; done'
          
          # Wait for canary API  
          timeout 60 bash -c 'until curl -f http://localhost:8001/healthz; do sleep 2; done'
          
          echo "âœ… All services healthy and ready for soak testing"
          
      - name: "ğŸ” Pre-soak health verification"
        run: |
          echo "ğŸ” Verifying service health before soak test..."
          
          # Test main API endpoints
          curl -f http://localhost:8000/health
          curl -f http://localhost:8000/healthz  
          curl -f http://localhost:8000/metrics
          
          # Test canary API
          curl -f http://localhost:8001/healthz
          
          # Quick orchestration test
          curl -X POST http://localhost:8000/orchestrate \
            -H "Content-Type: application/json" \
            -d '{"prompt": "Pre-soak test", "flags": []}'
          
          echo "âœ… Pre-soak verification complete"
          
      - name: "ğŸ“Š Baseline metrics collection"
        run: |
          echo "ğŸ“Š Collecting baseline metrics..."
          curl -s http://localhost:8000/metrics > baseline_metrics.txt
          echo "Baseline metrics collected at $(date)"
          
      - name: "ğŸ”¥ Locust Soak Test - Main API"
        run: |
          echo "ğŸ”¥ Starting 5-minute soak test with Locust..."
          
          # Create Locust test file for FastAPI soak testing
          cat > locust_soak.py << 'EOF'
          from locust import HttpUser, task, between, events
          import json
          import random
          
          class FastAPISoakUser(HttpUser):
              wait_time = between(0.1, 0.5)  # High frequency for soak testing
              
              def on_start(self):
                  """Initialize user session"""
                  # Health check on start
                  self.client.get("/healthz")
              
              @task(30)  # 30% of traffic
              def orchestrate_basic(self):
                  """Basic orchestration requests"""
                  prompts = [
                      "What is 2+2?",
                      "Explain quantum computing briefly",
                      "Generate a haiku about AI",
                      "Solve this: x + 5 = 10"
                  ]
                  
                  response = self.client.post("/orchestrate", json={
                      "prompt": random.choice(prompts),
                      "flags": [],
                      "temperature": random.uniform(0.5, 0.9)
                  })
                  
                  if response.status_code == 200:
                      data = response.json()
                      # Verify response structure
                      assert "response" in data
                      assert "latency_ms" in data
              
              @task(20)  # 20% of traffic
              def orchestrate_math(self):
                  """Math-focused requests with FLAG_MATH"""
                  math_prompts = [
                      "Calculate: 15 * 23 + 7",
                      "Solve: 3x^2 - 12x + 12 = 0", 
                      "What is the derivative of x^3?",
                      "Find the area of a circle with radius 5"
                  ]
                  
                  self.client.post("/orchestrate", json={
                      "prompt": random.choice(math_prompts),
                      "flags": ["FLAG_MATH"],
                      "temperature": 0.5
                  })
              
              @task(15)  # 15% of traffic
              def long_prompt_test(self):
                  """Long prompt stress testing"""
                  long_prompt = "This is a very long prompt for stress testing. " * 50
                  
                  self.client.post("/orchestrate", json={
                      "prompt": long_prompt,
                      "flags": [],
                      "temperature": 0.7
                  })
              
              @task(10)  # 10% of traffic
              def health_checks(self):
                  """Regular health monitoring"""
                  self.client.get("/health")
                  self.client.get("/healthz")
              
              @task(10)  # 10% of traffic  
              def admin_operations(self):
                  """Admin endpoint testing"""
                  models = ["models/math_adapter.bin", "models/general_adapter.bin"]
                  self.client.post("/admin/reload", json={
                      "lora": random.choice(models)
                  })
              
              @task(5)   # 5% of traffic
              def metrics_collection(self):
                  """Metrics endpoint monitoring"""
                  self.client.get("/metrics")
              
              @task(2)   # 2% error testing
              def intentional_errors(self):
                  """Test error handling and alerts"""
                  self.client.post("/test/error")
                  
          @events.request.add_listener  
          def my_request_handler(request_type, name, response_time, response_length, response, context, exception, **kwargs):
              """Custom request monitoring"""
              if exception:
                  print(f"âŒ Request failed: {request_type} {name} - {exception}")
              elif response.status_code >= 400:
                  print(f"âš ï¸ HTTP error: {request_type} {name} - {response.status_code}")
          EOF
          
          # Run Locust soak test with GitHub Action inputs
          locust -f locust_soak.py \
            --host=http://localhost:8000 \
            --users=${{ github.event.inputs.users || '150' }} \
            --spawn-rate=${{ github.event.inputs.spawn_rate || '25' }} \
            --run-time=${{ github.event.inputs.duration || '5m' }} \
            --headless \
            --csv=soak_results \
            --html=soak_report.html \
            --logfile=locust.log \
            --loglevel=INFO
            
      - name: "ğŸ¯ Canary Traffic Mirroring Test"
        run: |
          echo "ğŸ¯ Testing canary traffic mirroring (5% split)..."
          
          # Test direct canary access
          for i in {1..20}; do
            curl -s -H "X-Canary-Test: true" http://localhost:8001/orchestrate \
              -H "Content-Type: application/json" \
              -d '{"prompt": "Canary test '$i'", "flags": []}' > /dev/null
            sleep 0.1
          done
          
          echo "âœ… Canary mirroring test completed"
          
      - name: "ğŸ“ˆ Post-soak metrics analysis"
        run: |
          echo "ğŸ“ˆ Analyzing post-soak metrics..."
          
          # Collect final metrics
          curl -s http://localhost:8000/metrics > final_metrics.txt
          
          # Parse key metrics
          echo "=== SOAK TEST RESULTS ==="
          echo "ğŸ“Š Request metrics:"
          grep "swarm_api_requests_total" final_metrics.txt | head -5
          
          echo "âŒ Error metrics:"
          grep "swarm_api_5xx_total" final_metrics.txt || echo "No 5xx errors recorded"
          
          echo "â±ï¸ Latency metrics:"
          grep "swarm_api_request_duration_seconds" final_metrics.txt | grep "_sum\|_count" | head -3
          
          echo "ğŸ’¾ Memory metrics:"
          grep "swarm_api_memory_usage_bytes" final_metrics.txt
          
          # Validate thresholds
          total_requests=$(grep 'swarm_api_requests_total.*status="200"' final_metrics.txt | awk '{sum += $2} END {print sum}' || echo "0")
          total_errors=$(grep 'swarm_api_5xx_total' final_metrics.txt | awk '{sum += $2} END {print sum}' || echo "0")
          
          echo "ğŸ“ˆ Total successful requests: $total_requests"
          echo "âŒ Total 5xx errors: $total_errors"
          
          # Calculate error rate
          if [ "$total_requests" -gt "0" ]; then
            error_rate=$(echo "scale=3; $total_errors / ($total_requests + $total_errors) * 100" | bc -l 2>/dev/null || echo "0")
            echo "ğŸ“Š Error rate: ${error_rate}%"
            
            # Threshold validation
            if (( $(echo "$error_rate > 0.5" | bc -l) )); then
              echo "âŒ Error rate ${error_rate}% exceeds 0.5% threshold"
              exit 1
            fi
          fi
          
      - name: "ğŸš¨ Alert testing"
        run: |
          echo "ğŸš¨ Testing Prometheus alert triggers..."
          
          # Trigger intentional errors for alert testing
          for i in {1..5}; do
            curl -s -X POST http://localhost:8000/test/error || true
            sleep 1
          done
          
          echo "âœ… Alert trigger test completed"
          
      - name: "ğŸ“‹ Locust report analysis"
        run: |
          echo "ğŸ“‹ Analyzing Locust soak test results..."
          
          if [ -f "soak_results_stats.csv" ]; then
            echo "=== LOCUST PERFORMANCE SUMMARY ==="
            cat soak_results_stats.csv
            
            # Parse CSV for key metrics
            avg_response_time=$(tail -n +2 soak_results_stats.csv | grep -v "Aggregated" | awk -F',' '{sum+=$6; count++} END {print sum/count}' || echo "0")
            p95_response_time=$(tail -n +2 soak_results_stats.csv | grep "Aggregated" | awk -F',' '{print $9}' || echo "0")
            failure_rate=$(tail -n +2 soak_results_stats.csv | grep "Aggregated" | awk -F',' '{print $4}' || echo "0")
            
            echo "â±ï¸ Average response time: ${avg_response_time}ms"
            echo "ğŸ“Š 95th percentile: ${p95_response_time}ms" 
            echo "âŒ Failure rate: ${failure_rate}%"
            
            # Validate SLA thresholds
            if (( $(echo "$p95_response_time > 200" | bc -l) )); then
              echo "âŒ P95 latency ${p95_response_time}ms exceeds 200ms threshold"
              exit 1
            fi
            
            if (( $(echo "$failure_rate > 0.5" | bc -l) )); then
              echo "âŒ Failure rate ${failure_rate}% exceeds 0.5% threshold"  
              exit 1
            fi
            
            echo "âœ… All SLA thresholds met!"
          else
            echo "âš ï¸ Locust results file not found"
          fi
          
      - name: "ğŸ“¤ Upload artifacts"
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: soak-test-results
          path: |
            soak_report.html
            soak_results_*.csv
            locust.log
            baseline_metrics.txt
            final_metrics.txt
            
      - name: "ğŸ§¹ Cleanup"
        if: always()
        run: |
          echo "ğŸ§¹ Cleaning up containers..."
          docker-compose -f docker-compose.fastapi.yml down -v
          docker system prune -f
          
  summary:
    name: "ğŸ“Š Soak Test Summary"
    needs: soak-test
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: "âœ… Success notification"
        if: needs.soak-test.result == 'success'
        run: |
          echo "ğŸ‰ Soak test PASSED!"
          echo "âœ… P95 latency â‰¤ 200ms"
          echo "âœ… Error rate < 0.5%"
          echo "âœ… No memory fragmentation detected"
          echo "âœ… All health checks passing"
          
      - name: "âŒ Failure notification"  
        if: needs.soak-test.result == 'failure'
        run: |
          echo "ğŸ’¥ Soak test FAILED!"
          echo "âŒ Check artifacts for detailed analysis"
          echo "ğŸ” Review Locust report and metrics"
          exit 1 