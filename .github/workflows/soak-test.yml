name: "ðŸ”¥ FastAPI Soak Test"

on:
  workflow_dispatch:
    inputs:
      users:
        description: 'Number of concurrent users'
        required: false
        default: '150'
        type: string
      spawn_rate:
        description: 'User spawn rate per second' 
        required: false
        default: '25'
        type: string
      duration:
        description: 'Test duration (e.g., 5m, 30s)'
        required: false
        default: '5m'
        type: string
        
  schedule:
    # Run twice daily: 6 AM and 6 PM UTC 
    - cron: '0 6,18 * * *'
  
  push:
    paths:
    - 'docker-compose.fastapi.yml'
    - '.github/workflows/soak-test.yml'
    - 'api/**'
    - 'requirements.txt'
    
env:
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1

jobs:
  soak-test:
    name: "ðŸ”¥ 5-minute Soak Test"
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    outputs:
      test-status: ${{ steps.final-check.outputs.status }}
      p95-latency: ${{ steps.final-check.outputs.p95 }}
      error-rate: ${{ steps.final-check.outputs.error_rate }}
    
    steps:
      - name: "ðŸ“¥ Checkout repository"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          
      - name: "ðŸ”§ Install docker-compose v1 (fallback)"
        run: |
          echo "Installing docker-compose v1..."
          echo "Current PATH: $PATH"
          which docker || echo "Docker not found in PATH"
          docker --version || echo "Docker version failed"
          
          echo "Downloading docker-compose v1.29.2..."
          sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
          sudo chmod +x /usr/local/bin/docker-compose
          
          echo "Verifying installation..."
          which docker-compose || echo "docker-compose not found in PATH"
          docker-compose --version || echo "docker-compose version failed"
          ls -la /usr/local/bin/docker-compose || echo "docker-compose binary not found"
          echo "Docker Compose installation complete"
          
      - name: "ðŸ“ Create artifacts directory"
        run: |
          mkdir -p artifacts
          echo "Artifacts directory created: $(pwd)/artifacts"
          
      - name: "ðŸ Setup Python 3.11"
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          
      - name: "ðŸ“¦ Install Locust and dependencies"
        run: |
          pip install locust requests pytest
          
      - name: "ðŸ‹ Setup Docker Buildx"
        uses: docker/setup-buildx-action@v3
        
      - name: "ðŸš€ Start FastAPI Stack"
        run: |
          echo "ðŸš€ Starting FastAPI stack for soak testing..."
          
          echo "Debug: Checking docker-compose availability..."
          which docker-compose
          docker-compose --version
          
          echo "Debug: Checking compose file..."
          ls -la docker-compose.fastapi.yml
          
          echo "Debug: Starting containers..."
          docker-compose -f docker-compose.fastapi.yml up -d --build
          
          echo "â³ Waiting for services to become healthy..."
          sleep 30
          
          # Wait for main API
          timeout 60 bash -c 'until curl -f http://localhost:8000/healthz; do sleep 2; done'
          
          # Wait for canary API  
          timeout 60 bash -c 'until curl -f http://localhost:8001/healthz; do sleep 2; done'
          
          echo "âœ… All services healthy and ready for soak testing"
          
      - name: "ðŸ” Pre-soak health verification"
        run: |
          echo "ðŸ” Verifying service health before soak test..."
          
          # Test main API endpoints
          curl -f http://localhost:8000/health
          curl -f http://localhost:8000/healthz  
          curl -f http://localhost:8000/metrics
          
          # Test canary API
          curl -f http://localhost:8001/healthz
          
          # Quick orchestration test
          curl -X POST http://localhost:8000/orchestrate \
            -H "Content-Type: application/json" \
            -d '{"prompt": "Pre-soak test", "flags": []}'
          
          echo "âœ… Pre-soak verification complete"
          
      - name: "ðŸ“Š Baseline metrics collection"
        run: |
          echo "ðŸ“Š Collecting baseline metrics..."
          curl -s http://localhost:8000/metrics > artifacts/baseline_metrics.txt
          echo "Baseline metrics collected at $(date)"
          
      - name: "ðŸ”¥ Locust Soak Test - Main API"
        run: |
          echo "ðŸ”¥ Starting 5-minute soak test with Locust..."
          
          # Create Locust test file for FastAPI soak testing
          cat > locust_soak.py << 'EOF'
          from locust import HttpUser, task, between, events
          import json
          import random
          
          class FastAPISoakUser(HttpUser):
              wait_time = between(0.1, 0.5)  # High frequency for soak testing
              
              def on_start(self):
                  """Initialize user session"""
                  # Health check on start
                  self.client.get("/healthz")
              
              @task(30)  # 30% of traffic
              def orchestrate_basic(self):
                  """Basic orchestration requests"""
                  prompts = [
                      "What is 2+2?",
                      "Explain quantum computing briefly",
                      "Generate a haiku about AI",
                      "Solve this: x + 5 = 10"
                  ]
                  
                  response = self.client.post("/orchestrate", json={
                      "prompt": random.choice(prompts),
                      "flags": [],
                      "temperature": random.uniform(0.5, 0.9)
                  })
                  
                  if response.status_code == 200:
                      data = response.json()
                      # Verify response structure
                      assert "response" in data
                      assert "latency_ms" in data
              
              @task(20)  # 20% of traffic
              def orchestrate_math(self):
                  """Math-focused requests with FLAG_MATH"""
                  math_prompts = [
                      "Calculate: 15 * 23 + 7",
                      "Solve: 3x^2 - 12x + 12 = 0", 
                      "What is the derivative of x^3?",
                      "Find the area of a circle with radius 5"
                  ]
                  
                  self.client.post("/orchestrate", json={
                      "prompt": random.choice(math_prompts),
                      "flags": ["FLAG_MATH"],
                      "temperature": 0.5
                  })
              
              @task(15)  # 15% of traffic
              def long_prompt_test(self):
                  """Long prompt stress testing"""
                  long_prompt = "This is a very long prompt for stress testing. " * 50
                  
                  self.client.post("/orchestrate", json={
                      "prompt": long_prompt,
                      "flags": [],
                      "temperature": 0.7
                  })
              
              @task(10)  # 10% of traffic
              def health_checks(self):
                  """Regular health monitoring"""
                  self.client.get("/health")
                  self.client.get("/healthz")
              
              @task(10)  # 10% of traffic  
              def admin_operations(self):
                  """Admin endpoint testing"""
                  models = ["models/math_adapter.bin", "models/general_adapter.bin"]
                  self.client.post("/admin/reload", json={
                      "lora": random.choice(models)
                  })
              
              @task(5)   # 5% of traffic
              def metrics_collection(self):
                  """Metrics endpoint monitoring"""
                  self.client.get("/metrics")
              
              @task(2)   # 2% error testing
              def intentional_errors(self):
                  """Test error handling and alerts"""
                  self.client.post("/test/error")
                  
          @events.request.add_listener  
          def my_request_handler(request_type, name, response_time, response_length, response, context, exception, **kwargs):
              """Custom request monitoring"""
              if exception:
                  print(f"âŒ Request failed: {request_type} {name} - {exception}")
              elif response.status_code >= 400:
                  print(f"âš ï¸ HTTP error: {request_type} {name} - {response.status_code}")
          EOF
          
          # Run Locust soak test with GitHub Action inputs - output to artifacts
          locust -f locust_soak.py \
            --host=http://localhost:8000 \
            --users=${{ github.event.inputs.users || '150' }} \
            --spawn-rate=${{ github.event.inputs.spawn_rate || '25' }} \
            --run-time=${{ github.event.inputs.duration || '5m' }} \
            --headless \
            --csv=artifacts/soak_results \
            --html=artifacts/soak_report.html \
            --logfile=artifacts/locust.log \
            --loglevel=INFO
            
      - name: "ðŸ” Debug: Check Locust output immediately"
        run: |
          echo "=== POST-LOCUST DEBUG ==="
          echo "Checking if Locust created files..."
          
          echo "--- Artifacts directory after Locust ---"
          ls -la artifacts/ || echo "artifacts directory missing"
          
          echo "--- Expected files check ---"
          [ -f "artifacts/soak_report.html" ] && echo "âœ… soak_report.html found" || echo "âŒ soak_report.html missing"
          [ -f "artifacts/locust.log" ] && echo "âœ… locust.log found" || echo "âŒ locust.log missing"
          
          echo "--- CSV files check ---"
          ls -la artifacts/*.csv 2>/dev/null || echo "âŒ No CSV files in artifacts"
          
          echo "--- Any soak files in current directory? ---"
          ls -la soak* 2>/dev/null || echo "No soak files in current directory"
          
          echo "--- Locust working directory check ---"
          echo "Current PWD: $(pwd)"
          ls -la . | grep -E "(soak|locust)" || echo "No locust/soak files in current directory"
          
      - name: "ðŸŽ¯ Canary Traffic Mirroring Test"
        run: |
          echo "ðŸŽ¯ Testing canary traffic mirroring (5% split)..."
          
          # Test direct canary access
          for i in {1..20}; do
            curl -s -H "X-Canary-Test: true" http://localhost:8001/orchestrate \
              -H "Content-Type: application/json" \
              -d '{"prompt": "Canary test '$i'", "flags": []}' > /dev/null
            sleep 0.1
          done
          
          echo "âœ… Canary mirroring test completed"
          
      - name: "ðŸ“ˆ Post-soak metrics analysis"
        run: |
          echo "ðŸ“ˆ Analyzing post-soak metrics..."
          
          # Collect final metrics
          curl -s http://localhost:8000/metrics > artifacts/final_metrics.txt
          
          # Parse key metrics
          echo "=== SOAK TEST RESULTS ==="
          echo "ðŸ“Š Request metrics:"
          grep "swarm_api_requests_total" artifacts/final_metrics.txt | head -5
          
          echo "âŒ Error metrics:"
          grep "swarm_api_5xx_total" artifacts/final_metrics.txt || echo "No 5xx errors recorded"
          
          echo "â±ï¸ Latency metrics:"
          grep "swarm_api_request_duration_seconds" artifacts/final_metrics.txt | grep "_sum\|_count" | head -3
          
          echo "ðŸ’¾ Memory metrics:"
          grep "swarm_api_memory_usage_bytes" artifacts/final_metrics.txt
          
          # Validate thresholds
          total_requests=$(grep 'swarm_api_requests_total.*status="200"' artifacts/final_metrics.txt | awk '{sum += $2} END {print sum}' || echo "0")
          total_errors=$(grep 'swarm_api_5xx_total' artifacts/final_metrics.txt | awk '{sum += $2} END {print sum}' || echo "0")
          
          echo "ðŸ“ˆ Total successful requests: $total_requests"
          echo "âŒ Total 5xx errors: $total_errors"
          
          # Calculate error rate
          if [ "$total_requests" -gt "0" ]; then
            error_rate=$(echo "scale=3; $total_errors / ($total_requests + $total_errors) * 100" | bc -l 2>/dev/null || echo "0")
            echo "ðŸ“Š Error rate: ${error_rate}%"
            
            # Threshold validation
            if (( $(echo "$error_rate > 0.5" | bc -l) )); then
              echo "âŒ Error rate ${error_rate}% exceeds 0.5% threshold"
              exit 1
            fi
          fi
          
      - name: "ðŸš¨ Alert testing"
        run: |
          echo "ðŸš¨ Testing Prometheus alert triggers..."
          
          # Trigger intentional errors for alert testing
          for i in {1..5}; do
            curl -s -X POST http://localhost:8000/test/error || true
            sleep 1
          done
          
          echo "âœ… Alert trigger test completed"
          
      - name: "ðŸ“‹ Locust report analysis"
        run: |
          echo "ðŸ“‹ Analyzing Locust soak test results..."
          
          if [ -f "artifacts/soak_results_stats.csv" ]; then
            echo "=== LOCUST PERFORMANCE SUMMARY ==="
            cat artifacts/soak_results_stats.csv
            
            # Parse CSV for key metrics
            avg_response_time=$(tail -n +2 artifacts/soak_results_stats.csv | grep -v "Aggregated" | awk -F',' '{sum+=$6; count++} END {print sum/count}' || echo "0")
            p95_response_time=$(tail -n +2 artifacts/soak_results_stats.csv | grep "Aggregated" | awk -F',' '{print $9}' || echo "0")
            failure_rate=$(tail -n +2 artifacts/soak_results_stats.csv | grep "Aggregated" | awk -F',' '{print $4}' || echo "0")
            
            echo "â±ï¸ Average response time: ${avg_response_time}ms"
            echo "ðŸ“Š 95th percentile: ${p95_response_time}ms" 
            echo "âŒ Failure rate: ${failure_rate}%"
            
            # Validate SLA thresholds
            if (( $(echo "$p95_response_time > 200" | bc -l) )); then
              echo "âŒ P95 latency ${p95_response_time}ms exceeds 200ms threshold"
              exit 1
            fi
            
            if (( $(echo "$failure_rate > 0.5" | bc -l) )); then
              echo "âŒ Failure rate ${failure_rate}% exceeds 0.5% threshold"  
              exit 1
            fi
            
            echo "âœ… All SLA thresholds met!"
          else
            echo "âš ï¸ Locust results file not found"
          fi
          
      - name: "ðŸ” Debug: List artifact directory contents"
        if: always()
        run: |
          echo "=== ARTIFACT DIRECTORY DEBUG ==="
          echo "Current working directory: $(pwd)"
          echo "Workspace: ${{ github.workspace }}"
          
          echo "--- Directory structure ---"
          ls -la . || echo "Failed to list current directory"
          
          echo "--- Artifacts directory check ---"
          if [ -d "artifacts" ]; then
            echo "âœ… artifacts/ directory exists"
            echo "Contents of artifacts/:"
            ls -la artifacts/
            echo "Recursive listing:"
            find artifacts/ -type f -exec ls -la {} \; 2>/dev/null || echo "No files found in artifacts"
          else
            echo "âŒ artifacts/ directory does not exist"
          fi
          
          echo "--- Search for soak files anywhere ---"
          find . -name "*soak*" -type f 2>/dev/null || echo "No soak files found"
          find . -name "*.csv" -type f 2>/dev/null || echo "No CSV files found"
          find . -name "*.html" -type f 2>/dev/null || echo "No HTML files found"
          find . -name "*.log" -type f 2>/dev/null || echo "No log files found"
          
          echo "--- Current directory files ---"
          find . -maxdepth 2 -type f | head -20
          
      - name: "ðŸ“¤ Upload artifacts"
        uses: actions/upload-artifact@v4
        if: always()
        continue-on-error: true
        with:
          name: soak-test-results
          path: |
            artifacts/**
            *.csv
            *.html
            *.log
            **/soak*
          if-no-files-found: warn
            
      - name: "ðŸ§¹ Cleanup"
        if: always()
        run: |
          echo "ðŸ§¹ Cleaning up containers..."
          docker-compose -f docker-compose.fastapi.yml down -v
          docker system prune -f
          
  summary:
    name: "ðŸ“Š Soak Test Summary"
    needs: soak-test
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: "âœ… Success notification"
        if: needs.soak-test.result == 'success'
        run: |
          echo "ðŸŽ‰ Soak test PASSED!"
          echo "âœ… P95 latency â‰¤ 200ms"
          echo "âœ… Error rate < 0.5%"
          echo "âœ… No memory fragmentation detected"
          echo "âœ… All health checks passing"
          
      - name: "âŒ Failure notification"  
        if: needs.soak-test.result == 'failure'
        run: |
          echo "ðŸ’¥ Soak test FAILED!"
          echo "âŒ Check artifacts for detailed analysis"
          echo "ðŸ” Review Locust report and metrics"
          exit 1 