version: '3.8'

services:
  council-api:
    # No environment overrides - use defaults for real TinyLlama loading
    environment: {}

  # Guardian service for queue monitoring and auto-restart
  guardian:
    image: python:3.11-slim
    container_name: guardian
    command: >
      sh -c "
        pip install requests pyyaml &&
        python /app/guardian.py
      "
    environment:
      PROM_URL: "http://prometheus:9090"
      RULE_FILE: "/rules/self_repair_protocol.yaml"
      SLEEP_SECONDS: "15"
    volumes:
      - ./rules:/rules
      - ./guardian.py:/app/guardian.py
      - /var/run/docker.sock:/var/run/docker.sock
    restart: unless-stopped
    depends_on:
      - prometheus
    networks:
      - default

  mock-api:
    image: python:3.11-slim
    command: sh -c "pip install fastapi uvicorn && python /app/mockfast.py"
    volumes:
      - ./mockfast.py:/app/mockfast.py
    ports:
      - "9000:8000"
    environment:
      UVICORN_TIMEOUT_KEEP_ALIVE: "5"
    profiles: ["bypass"]

  # --- Canary service (5% traffic on GPU-1) ---
  api-canary:
    build: 
      context: .
      dockerfile: Dockerfile.canary
    container_name: api-canary
    ports:
      - "8001:8000"
    environment:
      GPU_AUX_ID: "1"                # ensure heavy models hit GPU-1
      CANARY_MODE: "true"            # flag used by Prom counters
      CUDA_VISIBLE_DEVICES: "1"
      REDIS_URL: "redis://redis:6379/0"
      SWARM_PROFILE: "canary"
      COUNCIL_TRAFFIC_PERCENT: 5
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ["1"]      # bind to GPU-1 only
        limits:
          memory: 8g
          cpus: "4.0"
    volumes:
      - ./memory:/app/memory
      - ./logs:/app/logs
    depends_on:
      - redis
    restart: on-failure:3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    networks:
      - default

  # Prometheus Pushgateway for chaos testing
  pushgw:
    image: prom/pushgateway:latest
    container_name: pushgateway
    ports:
      - "9091:9091"
    restart: on-failure:3
    command:
      - '--web.listen-address=0.0.0.0:9091'
      - '--web.telemetry-path=/metrics'
      - '--web.external-url=http://localhost:9091'
      - '--log.level=info'
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: "0.5"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9091/-/healthy"]
      interval: 15s
      timeout: 3s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "20m"
        max-file: "2"
    networks:
      - default

  # Reward model trainer - runs nightly at 02:00
  reward-trainer:
    build: .
    container_name: reward-trainer
    volumes:
      - ./preference_model:/app/preference_model
      - ./scripts:/app/scripts
      - reward_data:/opt/lumina/reward
    environment:
      - PYTHONUNBUFFERED=1
    depends_on:
      - redis
      - pushgw
    restart: unless-stopped
    command: >
      bash -c "
        apt-get update && apt-get install -y cron jq &&
        echo '0 2 * * * cd /app && bash scripts/nightly_reward_train.sh >> /var/log/reward_train.log 2>&1' | crontab - &&
        echo '0 * * * * cd /app && python preference_model/dataset.py >> /var/log/pair_builder.log 2>&1' | crontab - &&
        cron && tail -f /var/log/cron.log /var/log/reward_train.log /var/log/pair_builder.log
      "
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "3"
    networks:
      - default

  # Pattern-Miner service  
  pattern-miner:
    build: .
    command: python pattern_miner.py
    environment:
      CUDA_VISIBLE_DEVICES: "0"
      REDIS_URL: "redis://redis:6379/0"
    depends_on:
      - redis
    restart: on-failure:3
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "1.0"
    networks:
      - default

  # Real-time file change watcher
  inotify-watcher:
    image: alpine:3.20
    container_name: inotify-watcher
    volumes:
      - ./:/watch
      - file_metrics:/metrics
    working_dir: /watch
    restart: unless-stopped
    networks:
      - monitoring
    command: >
      sh -c "
      apk add --no-cache inotify-tools &&
      mkdir -p /metrics &&
      echo 'TITAN.EXE file watcher starting...' &&
      while inotifywait -rq -e modify,create,delete ./docs ./monitoring ./builder/templates ./rules ./config; do
        timestamp=$$(date +%s) &&
        echo 'file_change_total{path=\"live\",timestamp=\"'$$timestamp'\"} 1' > /metrics/file_watch.prom &&
        echo '['$$timestamp'] File change detected'
      done"

  # Textfile metrics exporter
  textfile-exporter:
    image: prom/node-exporter:latest
    container_name: textfile-exporter
    volumes:
      - file_metrics:/metrics:ro
    ports:
      - "9103:9103"
    restart: unless-stopped
    networks:
      - monitoring
    command:
      - '--path.textfile=/metrics'
      - '--web.listen-address=:9103'
      - '--collector.textfile'
      - '--no-collector.arp'
      - '--no-collector.bcache'
      - '--no-collector.bonding'
      - '--no-collector.conntrack'
      - '--no-collector.cpu'
      - '--no-collector.cpufreq'
      - '--no-collector.diskstats'
      - '--no-collector.edac'
      - '--no-collector.entropy'
      - '--no-collector.filefd'
      - '--no-collector.filesystem'
      - '--no-collector.hwmon'
      - '--no-collector.infiniband'
      - '--no-collector.ipvs'
      - '--no-collector.loadavg'
      - '--no-collector.mdadm'
      - '--no-collector.meminfo'
      - '--no-collector.netclass'
      - '--no-collector.netdev'
      - '--no-collector.netstat'
      - '--no-collector.nfs'
      - '--no-collector.nfsd'
      - '--no-collector.pressure'
      - '--no-collector.sockstat'
      - '--no-collector.stat'
      - '--no-collector.time'
      - '--no-collector.timex'
      - '--no-collector.uname'
      - '--no-collector.vmstat'
      - '--no-collector.xfs'
      - '--no-collector.zfs'

  guide-loader:
    image: python:3.11-slim
    container_name: guide-loader
    restart: unless-stopped
    volumes:
      - ./:/app
    working_dir: /app
    ports:
      - "9108:9108"
    environment:
      - GUIDE_METRICS_PORT=9108
    networks:
      - monitoring
    command: >
      sh -c "
        pip install prometheus-client watchdog requests &&
        python guide_loader/main.py
      "
    depends_on:
      - prometheus

volumes:
  reward_data:
  file_metrics:

networks:
  monitoring:
    external: true 