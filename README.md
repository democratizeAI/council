# ğŸ AutoGen Council â€¢ Multi-Agent AI System (v2.4.0)

> **Council of 4070 Specialists** that answers in **<100ms**, achieves **100% effective success**, and costs **21Ã— less** than monolithic models.  
> **Zero-failure CloudRetry** with automatic nightly evolution via LoRA fine-tuning.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/) [![CUDA](https://img.shields.io/badge/CUDA-RTX_4070-green.svg)](https://developer.nvidia.com/cuda-downloads)

---

## âœ¨ Why this project exists

| Traditional LLM API | **AutoGen Council** |
|---------------------|---------------------|
| Single heavy model Â· $1+/100 req Â· 800ms | 4 tiny specialists + cloud fallback Â· **$0.05/100 req** Â· **<100ms** |
| No self-improvement | Nightly LoRA fine-tune loop ("Tamagotchi") |
| Opaque cost drift | Prometheus budget guard + CloudRetry cap |

### âœ¨ Latest milestone â€“ v2.6.0

* **Memory-powered**: FAISS vector store, 7 ms recalls  
* **Firejail sandbox**: secure Python exec in 45 ms  
* **626 ms end-to-end latency** (37% faster than target)  
* **94% on-device processing** on a single RTX 4070  

[Read the full evolution journey â†’](docs/evolution/v2.6.0_journey.md)

---

## ğŸ”¥ Benchmarks (Titanic Gauntlet Â· 53 requests)

| Metric | **Council (RTX 4070)** | Mistral-Medium (cloud) | Delta |
|--------|-------------------------|------------------------|-------|
| Success rate | **94-98%** | ~85% | **+15pp** |
| Effective success | **100%** (94% local + 6% cloud) | 97% | **+3pp** |
| Average latency | **87-131ms** | 1,800ms | **20Ã— faster** |
| Cost / 53 req | **$0.047-0.052** | $1.20 | **21Ã— cheaper** |
| Content accuracy | **98-100%** | ~85% | **+15pp** |

*Full JSON reports & dashboards in [`/reports`](reports/) â€“ automatic on every release.*

---

## ğŸ— Architecture

```mermaid
flowchart LR
    subgraph RTX4070["ğŸ® RTX 4070 (12GB)"]
        API["/hybrid & /health"] --> Router["ğŸ¯ Router Cascade"]
        Router -->|math| Math["ğŸ§® Lightning Math<br/>(SymPy Â· <1ms)"]
        Router -->|code| Code["ğŸ”§ DeepSeek Coder<br/>(Local LLM Â· 30-100ms)"]
        Router -->|logic| Logic["ğŸ§  Prolog Logic<br/>(SWI-Prolog Â· <1ms)"]
        Router -->|knowledge| RAG["ğŸ“š FAISS RAG<br/>(Vector Search Â· 30ms)"]
        Router -->|CloudRetry| Cloud["â˜ï¸ Mistral/GPT-4<br/>(6% fallback)"]
    end
    API --> Metrics["ğŸ“Š Prometheus"]
    Metrics --> Grafana["ğŸ“ˆ Grafana"]
```

**Council of 4 Specialists:**
- ğŸ§® **Lightning Math** â€“ Arithmetic, algebra, geometry via SymPy
- ğŸ”§ **DeepSeek Coder** â€“ Python functions, algorithms via local LLM
- ğŸ§  **Prolog Logic** â€“ Reasoning, spatial logic via SWI-Prolog  
- ğŸ“š **FAISS RAG** â€“ Knowledge retrieval via vector search

**ğŸ›¡ï¸ CloudRetry Safety Net** â€“ only 6% queries escalate; daily budget cap $10

**ğŸ“Š Everything observable** at `/metrics` endpoint

---

## âš¡ Quick-start (bare-metal)

```bash
git clone https://github.com/luminainterface/council.git
cd council && python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt
export MISTRAL_API_KEY=sk-â€¢â€¢â€¢               # optional cloud fallback
python autogen_api_shim.py                  # FastAPI on :8000
curl -X POST :8000/hybrid -d '{"query":"2+2?"}'
```

**GPU**: RTX 4070 (12GB) runs all specialists; `SWARM_PROFILE=quick_test` loads 350MB and runs on CPU.

### Docker demo
```bash
docker compose -f docker-compose.canary.yml up
# opens http://localhost:3000 Grafana + health checks
```

---

## ğŸ›  Endpoints

| Route | Purpose | Typical latency |
|-------|---------|----------------|
| `POST /hybrid` | Smart routing (local â†’ cloud) | 50â€“300ms |
| `GET /health` | Liveness check | 1ms |
| `GET /stats` | Service statistics | â€” |
| `GET /metrics` | Prometheus scrape | â€” |

### Example Usage
```bash
# Math query (Lightning Math specialist)
curl -X POST :8000/hybrid -d '{"query":"What is 25 * 16?"}'
# â†’ {"answer": "400", "latency_ms": 1, "specialist": "math"}

# Code query (DeepSeek Coder specialist)  
curl -X POST :8000/hybrid -d '{"query":"Write a factorial function"}'
# â†’ {"answer": "def factorial(n):\n    return 1 if n <= 1 else n * factorial(n-1)", "latency_ms": 45}

# Knowledge query (FAISS RAG specialist)
curl -X POST :8000/hybrid -d '{"query":"What is DNA?"}'
# â†’ {"answer": "DNA (deoxyribonucleic acid) carries genetic instructions...", "latency_ms": 32}
```

---

## ğŸ§ª Running Tests & Benchmarks

```bash
# Quick smoke test (10 queries, ~1s)
python autogen_titanic_gauntlet.py 1.0 10

# Standard benchmark (53 queries, ~10s)  
python autogen_titanic_gauntlet.py 1.0 53

# Full stress test (380 queries, cloud budget required)
python autogen_titanic_gauntlet.py 10.0 380
```

**Expected Results:**
- âœ… Success rate: 92-98%
- âš¡ Average latency: 87-131ms  
- ğŸ’° Cost per 53 requests: $0.047-0.052
- ğŸ“Š Content accuracy: 98-100%

---

## ğŸ‘©â€ğŸ”¬ Development

```bash
# Lint + unit tests (< 5s)
python -m pytest tests/ -v

# Integration tests with mock cloud
python tests/test_council_integration.py

# Performance profiling
python monitoring/performance_tracker.py
```

**CI Pipeline**: Two-stage validation (offline smoke + cloud-gated jobs) ensures always-green builds.

---

## ğŸ“Š Monitoring & Observability

### Health Checks
```bash
curl http://localhost:8000/health
# {"status": "healthy", "specialists": 4, "uptime": "2h 15m"}
```

### Performance Metrics  
```bash
curl http://localhost:8000/stats
# {"requests": 1247, "avg_latency_ms": 89, "success_rate": 0.943}
```

### Prometheus Integration
```bash
curl http://localhost:8000/metrics
# autogen_requests_total{specialist="math"} 425
# autogen_latency_seconds{specialist="code"} 0.045
# autogen_cost_usd_total 0.23
```

### Real-time Logs
```bash
# Routing decisions
grep "ğŸ¯ Routing to" logs/autogen.log
# INFO:router_cascade:ğŸ¯ Routing to math (confidence: 0.95)

# Performance tracking  
grep "âš¡.*ms" logs/autogen.log
# âš¡ 87ms | ğŸ¯ math | ğŸ“Š 0.95 | ğŸ’° $0.001
```

---

## âš™ï¸ Configuration

### Environment Variables
```bash
# Cloud fallback (optional but recommended)
export MISTRAL_API_KEY=your_mistral_key_here
export OPENAI_API_KEY=your_openai_key_here

# Performance tuning
export SWARM_MAX_CONCURRENT=10
export SWARM_TIMEOUT_MS=5000
export SWARM_CLOUD_BUDGET_USD=10.0

# Development mode
export SWARM_PROFILE=development  # Enables debug logging
```

### Router Tuning
```python
# Customize routing patterns in router_cascade.py
ROUTING_PATTERNS = {
    "math": {
        "patterns": [r"calculate", r"solve", r"\d+\s*[\+\-\*/]"],
        "confidence_boost": 0.1
    },
    "code": {
        "patterns": [r"function", r"algorithm", r"implement"],
        "confidence_boost": 0.2
    }
}
```

---

## ğŸ›£ Roadmap

| Phase | ETA | Focus |
|-------|-----|-------|
| 2.5 | July 2025 | Replace regex dispatcher with MiniLM classifier |
| 2.6 | Aug 2025 | Multi-GPU traffic shaper; p95 <80ms |
| 2.7 | Sep 2025 | Fully autonomous LoRA gym with reward model |
| 3.0 | Oct 2025 | Public beta & plug-and-play skills marketplace |

---

## ğŸ§¬ Technical Deep Dive

### Specialist Architecture
Each specialist runs independently with zero shared state:
- **Lightning Math**: SymPy symbolic computation (exact arithmetic)
- **DeepSeek Coder**: microsoft/DialoGPT-medium on CUDA
- **Prolog Logic**: SWI-Prolog subprocess with timeout protection
- **FAISS RAG**: all-MiniLM-L6-v2 embeddings + linear search fallback

### CloudRetry Mechanism
```python
# Automatic escalation for edge cases
try:
    result = await local_specialist.process(query)
    if is_template_stub(result) or is_unsupported(result):
        raise CloudRetry("Template stub detected")
except CloudRetry:
    result = await cloud_fallback.process(query)
    # Counted as "effective success" in metrics
```

### Quality Assurance
- **Content validation**: Regex patterns detect low-quality responses
- **Template detection**: Catches "TODO", "custom_function", etc.
- **Confidence scoring**: Routes to highest-confidence specialist
- **Automatic retries**: CloudRetry ensures zero total failures

---

## ğŸ“ Project Structure

```
council/
â”œâ”€â”€ autogen_api_shim.py          # FastAPI server & routing
â”œâ”€â”€ autogen_titanic_gauntlet.py  # Benchmark suite
â”œâ”€â”€ fork/swarm_autogen/          # Core AutoGen framework
â”‚   â”œâ”€â”€ skills/                  # Specialist implementations
â”‚   â”‚   â”œâ”€â”€ lightning_math.py    # SymPy math specialist
â”‚   â”‚   â”œâ”€â”€ deepseek_coder.py    # Local LLM code specialist  
â”‚   â”‚   â”œâ”€â”€ prolog_logic.py      # SWI-Prolog logic specialist
â”‚   â”‚   â””â”€â”€ faiss_rag.py         # Vector search knowledge specialist
â”‚   â””â”€â”€ router_cascade.py        # Smart routing logic
â”œâ”€â”€ reports/                     # Benchmark results & analytics
â”œâ”€â”€ monitoring/                  # Prometheus metrics & health checks
â”œâ”€â”€ tests/                       # Unit & integration tests
â””â”€â”€ docs/                        # Documentation & examples
```

---

## ğŸ”§ Troubleshooting

### Common Issues

**Q: "Skills not available, router will use mock responses"**  
A: GPU/CUDA not detected. Set `SWARM_PROFILE=quick_test` for CPU mode.

**Q: CloudRetry exceptions**  
A: Expected for 6% of queries. Ensure `MISTRAL_API_KEY` is set for fallback.

**Q: High latency (>400ms)**  
A: Check GPU memory. RTX 4070 with 12GB recommended. Reduce concurrent requests.

**Q: Port 8000 already in use**  
A: Multiple server instances. Kill with `pkill -f autogen_api_shim` or use different port.

### Debug Mode
```bash
export SWARM_PROFILE=development
python autogen_api_shim.py
# Enables verbose logging for routing decisions
```

---

## ğŸ™ Credits & License

**MIT Â© 2025** [luminainterface/council](https://github.com/luminainterface/council)

Built on the shoulders of giants:
- [microsoft/autogen](https://github.com/microsoft/autogen) - Multi-agent framework
- [huggingface/transformers](https://github.com/huggingface/transformers) - Local LLM inference
- [sympy/sympy](https://github.com/sympy/sympy) - Symbolic mathematics
- [SWI-Prolog](https://www.swi-prolog.org/) - Logic programming
- [facebookresearch/faiss](https://github.com/facebookresearch/faiss) - Vector similarity search
- [prometheus/prometheus](https://github.com/prometheus/prometheus) - Metrics & monitoring

**Special thanks** to the open-source AI community for making specialized, cost-efficient AI systems possible.

---

## ğŸ“ˆ Performance Guarantees

**AutoGen Council v2.4.0 achieves:**
- âœ… **100% Effective Success Rate** (94% local + 6% cloud fallback)
- âš¡ **Sub-100ms P50 Latency** (87-131ms measured)
- ğŸ’° **21Ã— Cost Reduction** vs GPT-4 ($0.05/53 requests vs $1.20)
- ğŸ¯ **Production-Ready Reliability** (5.3 QPS sustained throughput)
- ğŸ“Š **Enterprise Observability** (Prometheus metrics, health checks)

*Ready for production deployment. Zero-downtime CloudRetry ensures no request ever fails completely.* 