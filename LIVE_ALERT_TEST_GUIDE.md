# ðŸ§ª Live Alert Test Guide

## ðŸŽ¯ Goal
Verify your alert pipeline works end-to-end by triggering real VRAM spikes and watching alerts fire in the correct escalation sequence.

---

## ðŸš€ Quick Start

### **Option A: Full Escalation Test (7 minutes)**
Tests all three levels: Warning â†’ Critical â†’ Emergency

```bash
make -f Makefile.day2 test-alerts-e2e
```

### **Option B: Single Threshold Test (3 minutes)**
Test one specific alert threshold

```bash
# Test warning alert (75% threshold)
make -f Makefile.day2 test-alerts-single VRAM=78

# Test critical alert (85% threshold) 
make -f Makefile.day2 test-alerts-single VRAM=88

# Test emergency alert (95% threshold)
make -f Makefile.day2 test-alerts-single VRAM=97
```

---

## ðŸ“‹ Pre-Test Checklist

### **1. Verify Monitoring Stack is Running**
```bash
# Check services are up
make -f Makefile.day2 health

# Verify Prometheus is accessible
curl http://localhost:9090/api/v1/status/config

# Verify AlertManager is accessible  
curl http://localhost:9093/api/v1/status
```

### **2. Configure Notification Channels**
Edit `monitoring/alertmanager.yml` and replace:
- `REPLACE_WITH_SLACK_WEBHOOK_URL` â†’ Your Slack webhook
- `REPLACE_WITH_PAGERDUTY_INTEGRATION_KEY` â†’ Your PagerDuty key

### **3. Load Alert Rules**
```bash
# Reload configurations
curl -X POST http://localhost:9090/-/reload
curl -X POST http://localhost:9093/-/reload

# Verify rules are loaded
make -f Makefile.day2 validate-alert-config
```

---

## ðŸ§ª Test Execution

### **Step 1: Start the Test**
```bash
make -f Makefile.day2 test-alerts-e2e
```

### **Step 2: Expected Timeline**
```
00:00 - Test starts, monitoring begins
00:05 - VRAM spike to 78% (Warning threshold)
03:00 - Alert "VRAMWarning" should fire
03:05 - VRAM spike to 88% (Critical threshold)  
05:00 - Alert "VRAMCritical" should fire
05:05 - VRAM spike to 97% (Emergency threshold)
05:30 - Alert "VRAMEmergency" should fire
07:30 - Test completes, cleanup begins
```

### **Step 3: Watch for Notifications**

**Slack #swarm-ops:**
```
âš ï¸ Warning: VRAMWarning
GPU memory usage is 78.0% (>75%). Monitor for potential allocation issues.

ðŸš¨ Critical: VRAMCritical  
GPU memory usage is 88.0% (>85%). Immediate attention required to prevent OOM failures.
```

**Slack #swarm-incidents:**
```
ðŸš¨ URGENT: VRAMEmergency
GPU memory usage is 97.0% (>95%). OOM failure imminent. Immediate intervention required.
```

**Email (swarm-ops@company.com):**
```
Subject: ðŸš¨ SwarmAI Critical Alert: VRAMCritical
Critical alert fired in SwarmAI production environment...
```

**PagerDuty:**
```
New Incident: ðŸš¨ VRAMEmergency - URGENT
Environment: production
Component: gpu-memory
```

---

## ðŸ“Š Verification Points

### **During Test - Check Grafana**
1. Open http://localhost:3000
2. Navigate to "SwarmAI Day-2 Operations Dashboard"
3. Watch the "VRAM Usage %" panel spike to target levels
4. Verify colors change: Green â†’ Yellow â†’ Red

### **During Test - Check Prometheus**
```bash
# Check current alerts
make -f Makefile.day2 show-alerts

# Expected output:
# VRAMWarning (warning) - VRAM usage elevated
# VRAMCritical (critical) - VRAM usage critically high  
# VRAMEmergency (page) - VRAM EMERGENCY - OOM imminent
```

### **During Test - Check AlertManager**
```bash
# Check active alerts
curl -s http://localhost:9093/api/v1/alerts | jq '.data[].labels.alertname'

# Check silences (should be empty)
make -f Makefile.day2 show-silences
```

---

## âœ… Success Criteria

### **Alert Firing**
- âœ… VRAMWarning fires when usage > 75%
- âœ… VRAMCritical fires when usage > 85%  
- âœ… VRAMEmergency fires when usage > 95%
- âœ… Correct escalation sequence (Warning â†’ Critical â†’ Emergency)

### **Notification Delivery**  
- âœ… Slack #swarm-ops receives warning + critical alerts
- âœ… Slack #swarm-incidents receives emergency alerts
- âœ… Email sent for critical alerts
- âœ… PagerDuty incident created for emergency

### **Alert Content**
- âœ… Alerts include runbook URLs
- âœ… Descriptions explain the issue clearly
- âœ… Severity levels match expectation
- âœ… Environment and component labels correct

---

## ðŸ”§ Troubleshooting

### **No Alerts Firing**
```bash
# Check Prometheus is scraping metrics
curl "http://localhost:9090/api/v1/query?query=up"

# Check alert rules are loaded
curl http://localhost:9090/api/v1/rules | jq '.data.groups[].name'

# Verify GPU detection
python scripts/chaos_gpu.py --vram 50 --duration 10
```

### **Alerts Firing but No Notifications**
```bash
# Check AlertManager routing
curl http://localhost:9093/api/v1/routes

# Check receiver configuration
curl http://localhost:9093/api/v1/receivers

# Verify webhook URLs in alertmanager.yml
grep -n "REPLACE_WITH" monitoring/alertmanager.yml
```

### **Wrong Alert Thresholds**
```bash
# Check current thresholds
grep -A5 "VRAMWarning\|VRAMCritical\|VRAMEmergency" monitoring/alerts.yml

# Tune if needed
make -f Makefile.day2 tune-alert-thresholds
```

---

## ðŸ§¹ Cleanup

### **Stop Any Running Tests**
```bash
# Stop chaos tests
make -f Makefile.day2 clear-chaos

# Silence test alerts
make -f Makefile.day2 silence-test-alerts
```

### **Check System Recovery**
```bash
# Verify alerts have resolved
make -f Makefile.day2 show-alerts

# Check Grafana dashboard is green
# Check VRAM usage is back to normal levels
```

---

## ðŸ“‹ Test Report Template

After running the test, document results:

```
ðŸ§ª Alert Pipeline Test Results - [DATE]
===============================================

âœ… Test Execution:
- Full escalation test completed: [YES/NO]
- Duration: [X] minutes
- Chaos scenarios: VRAM 78% â†’ 88% â†’ 97%

âœ… Alert Firing:
- VRAMWarning (75%): [FIRED/NOT FIRED] at [TIME]
- VRAMCritical (85%): [FIRED/NOT FIRED] at [TIME]  
- VRAMEmergency (95%): [FIRED/NOT FIRED] at [TIME]
- Escalation sequence: [CORRECT/INCORRECT]

âœ… Notification Delivery:
- Slack #swarm-ops: [RECEIVED/NOT RECEIVED]
- Slack #swarm-incidents: [RECEIVED/NOT RECEIVED]
- Email notifications: [RECEIVED/NOT RECEIVED]
- PagerDuty incidents: [CREATED/NOT CREATED]

ðŸ”§ Issues Found:
- [List any issues or misconfigurations]

ðŸŽ¯ Next Actions:
- [Any threshold adjustments needed]
- [Notification channel fixes required]
- [Additional testing needed]
```

---

## ðŸŽ‰ Success!

If all tests pass, you now have **battle-tested alerts** that:

âœ… **Fire at the right thresholds** with proper burn-in times  
âœ… **Escalate correctly** from warning â†’ critical â†’ emergency  
âœ… **Deliver notifications** to the right channels  
âœ… **Provide actionable context** with runbook links  
âœ… **Reduce noise** with inhibit rules  

Your production monitoring is now **proven to work** when it matters most! ðŸš€

Remember: Test your alerts quarterly to ensure they stay reliable as your system evolves. ðŸ“… 