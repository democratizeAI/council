# üöÄ SwarmAI - Advanced AI Testing & Evaluation Platform

[![Unit Tests & Linting](https://github.com/luminainterface/swarmAI/actions/workflows/test.yml/badge.svg?branch=master)](https://github.com/luminainterface/swarmAI/actions/workflows/test.yml)
[![Leak Suite Status](https://github.com/luminainterface/swarmAI/actions/workflows/leak_suite.yml/badge.svg?branch=evolution-main)](https://github.com/luminainterface/swarmAI/actions/workflows/leak_suite.yml?query=branch%3Aevolution-main)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![NEXUS Version](https://img.shields.io/badge/NEXUS-v0.0.2-brightgreen.svg)](https://github.com/luminainterface/swarmAI/releases/tag/v0.0.2)

## üî•üíÄ‚öîÔ∏è NEXUS Testing Suite - Sprint S-5 Advanced Evaluation ‚öîÔ∏èüíÄüî•

**Ultra-high-precision AI evaluation framework targeting Mistral-Large-tier accuracy (‚â•92%)**

### üöÄ Quick Start - NEXUS

```bash
# Clone and setup NEXUS
git clone https://github.com/luminainterface/swarmAI.git
cd swarmAI
pip install -r nexus_requirements.txt

# Start NEXUS server
python serve.py

# Run advanced evaluation
python tools/nexus_rapid_test.py
```

## üß† **MISTRAL INTEGRATION COMPLETE** - Autonomous Evolution Ready ‚úÖ

**Status:** ‚úÖ **FULLY OPERATIONAL**  
**Integration Date:** 2024-01-22T17:00:00Z  
**Test Results:** 8/8 PASS  
**Council Status:** 10-Member Democratic AI Evolution  

### üéØ **What's New: True Autonomous Evolution**

SwarmAI has evolved from **directed adaptation** to **genuine autonomous evolution** with the integration of **Mistral-0.5B-Instruct** as the 10th council member:

```yaml
Council Composition:
  Emotional Members: 9 (Joy, Fear, Anger, Sadness, Disgust, Surprise, Trust, Anticipation, Determination)
  Logical Member: 1 (Reason - Mistral Reasoner v1)
  Voting System: Democratic consensus with evidence-based decision making
  Evolution Cycle: Nightly autonomous training and deployment
```

### üîÑ **Autonomous Evolution Cycle**

```mermaid
graph TD
    A[Auto-Crawler] -->|Challenge Discovery| B[Dataset Creation]
    B -->|Nightly 02:00 UTC| C[LoRA Training]
    C -->|mistral_logic_boost_v1| D[Validation Testing]
    D -->|Fitness Evaluation| E[Council Vote]
    E -->|10-Member Democracy| F{Consensus?}
    F -->|Yes: Promote| G[Production Deployment]
    F -->|No: Rollback| H[Safety Fallback]
    G -->|Success| I[Genetic Crossover]
    H -->|Retry| C
    I -->|Evolution| A
```

### üß¨ **Integration Components**

| Component | Status | Purpose |
|-----------|--------|---------|
| **Agent Shell** | ‚úÖ `agents/mistral_reasoner.yaml` | Logical reasoning specialist with 4 routing patterns |
| **Council Integration** | ‚úÖ `evolution_policy.yaml` | Equal voting weight (1.0) in 10-member democracy |
| **LoRA Pipeline** | ‚úÖ `jobs/queue/mistral_logic_boost_v1.yaml` | Queued for nightly evolution cycle |
| **Knowledge Base** | ‚úÖ `knowledge_cards/mistral_reasoner_v1.md` | Auto-updating documentation with lineage tracking |
| **Monitoring** | ‚úÖ Prometheus + Grafana | Real-time metrics: `mistral_tokens_total`, `mistral_errors_total` |

### üéõÔ∏è **Performance Targets**

```yaml
Current Baselines:
  Logic Pass Rate: 72% ‚Üí Target: 77%+
  Response Latency: 35ms ‚Üí Target: <40ms p95  
  Accuracy: 78% ‚Üí Target: 80%+
  Consistency: 85% ‚Üí Target: 90%+

Resource Allocation:
  VRAM Budget: 320 MB (Q4_K_M quantization)
  Training Time: 5-6 hours per cycle
  Fitness Threshold: 0.75+ for promotion
```

### üö® **Production Monitoring**

**Key Metrics to Watch:**
- **Council Votes**: Check `logs/evolution_votes.log` for democratic decisions
- **Training Success**: Monitor `jobs/complete/mistral_logic_boost_v1/results.json`
- **Routing Efficiency**: Mistral should handle 5-15% of logical reasoning queries
- **Consensus Time**: Council decisions should complete in <5 seconds

**Health Checks:**
```bash
# Verify integration status
python test_mistral_integration.py

# Check council health
curl http://localhost:8000/api/agents/mistral_reasoner/health

# Monitor evolution progress
curl http://localhost:8000/api/evolution/status
```

### üî¨ **What Makes This "Truly Evolutionary"**

Unlike traditional AI systems with static responses, SwarmAI now features:

1. **Genetic Emotional Intelligence**: Emotions that evolve their decision-making patterns
2. **Emergent Challenge Discovery**: System discovers its own training challenges
3. **Democratic Consensus**: AI agents vote on their own evolution
4. **Closed-Loop Learning**: Success patterns automatically propagate to other agents
5. **Autonomous Rollback**: System protects itself from harmful mutations

**This is genuine autonomous evolution** - the system improves itself without human intervention while maintaining safety through democratic oversight.

### üß† Core NEXUS Components

| Component | Purpose | Performance |
|-----------|---------|-------------|
| `serve.py` | Multi-engine API server with intelligent routing | <100ms response time |
| `lightning_math_engine.py` | High-speed mathematical computation | <1ms factorial, <5ms complex calc |
| `deepseek_coder_engine.py` | Advanced code generation with execution testing | 95%+ code success rate |
| `faiss_knowledge_engine.py` | Semantic knowledge retrieval with vector search | Sub-50ms retrieval |
| `prolog_logic_engine.py` | Logic reasoning and inference engine | Formal logic validation |
| `router_intent.py` | 570x faster query classification | <10ms routing decision |

### üéØ Sprint S-5 Advanced Evaluation Features

#### üßÆ **Semantic Evaluation**
- **Sentence-Transformers**: Real semantic similarity scoring
- **ROUGE-L Metrics**: Text quality assessment beyond keywords
- **Code Execution Testing**: Actual Python code compilation and execution
- **Mathematical Precision**: Numeric validation with floating-point tolerance

#### üî¨ **Multi-Modal Scoring**
```python
# Advanced evaluation pipeline
evaluation_result = {
    "exact_match": bool,           # Exact keyword/pattern match
    "numeric_match": bool,         # Mathematical precision match
    "semantic_similarity": float,  # 0.0-1.0 similarity score
    "rouge_score": float,          # ROUGE-L text quality
    "code_executes": bool,         # Actual code execution test
    "overall_score": float         # Combined weighted score
}
```

#### üìä **Performance Thresholds**
- **Basic accuracy**: 70% threshold for pass/fail
- **Advanced accuracy**: Semantic + execution + ROUGE combined
- **Mistral-Large-tier**: ‚â•92% target for production deployment
- **Confidence intervals**: Wilson score with 95% confidence

### üß™ NEXUS Test Suite

```bash
# Run comprehensive NEXUS evaluation
python tools/nexus_rapid_test.py --math 20 --code 15 --knowledge 10 --logic 10

# Quick validation test
python tools/quick_nexus_test.py

# Boss-level comprehensive testing
python tools/boss_test.py
```

#### üìà **Test Categories**

| Category | Tests | Advanced Features |
|----------|-------|-------------------|
| **Math** | Factorials, percentages, exponents, calculus | SymPy CAS integration, symbolic math |
| **Code** | Algorithm implementation, data structures | Syntax validation, execution testing |
| **Knowledge** | Factual recall, AI/ML knowledge | FAISS semantic retrieval |
| **Logic** | Temporal reasoning, syllogisms, sequences | Formal logic validation |

### üîß NEXUS Architecture

```mermaid
graph TD
    A[Query Input] --> B[Router Intent Classification]
    B --> C{Query Type}
    C -->|Math| D[Lightning Math Engine]
    C -->|Code| E[DeepSeek Coder Engine]
    C -->|Knowledge| F[FAISS Knowledge Engine]
    C -->|Logic| G[Prolog Logic Engine]
    D --> H[Advanced Evaluation]
    E --> H
    F --> H
    G --> H
    H --> I[Multi-Modal Scoring]
    I --> J[Response + Confidence]
```

### üéñÔ∏è **Sprint S-5 Results Example**

```
üî•üíÄ‚öîÔ∏è NEXUS SPRINT S-5 ADVANCED EVALUATION TEST ‚öîÔ∏èüíÄüî•
======================================================================
‚ö° BASIC ACCURACY: 87.3% (48/55)
‚ö° ADVANCED ACCURACY: 85.2% (semantic + execution)
‚ö° AVERAGE LATENCY: 127.8ms
‚ö° TOTAL TEST TIME: 12.3s

üìä CONFIDENCE INTERVALS (95%):
   Basic: 75.8% - 94.2%
   Advanced: 73.1% - 92.8%

üìä BLOCK BREAKDOWN:
   üßÆ Math:      95.0% (20 queries)
   üíª Code:      86.7% (15 queries)
   üìö Knowledge: 80.0% (10 queries)
   üß† Logic:     80.0% (10 queries)

üéØ SPRINT S-5 PERFORMANCE ASSESSMENT:
üü° APPROACHING MISTRAL-LARGE-TIER (88-92%)
```

### üì¶ Installation & Dependencies

```bash
# Core NEXUS requirements
pip install fastapi uvicorn pydantic requests

# Advanced evaluation dependencies
pip install sentence-transformers rouge-score numpy

# Optional: Symbolic math (SymPy CAS)
pip install sympy

# Optional: Vector search (FAISS)
pip install faiss-cpu

# Development testing
pip install pytest pytest-asyncio
```

---

## ü™¥ Tamagotchi Evolution System

üß†‚ö° **Sub-10ms Emotional Consensus | 570x Performance Improvement | Democratic AI Evolution**

## üîçüíÄ Nuclear-Grade Validation Certified

This project includes a **brutal "no-excuses" leak test suite** that provides **iron-clad proof** of genuine AI performance gains:

```bash
# üîí Run the nuclear validation gauntlet (35-45 minutes)
make -f Makefile.leak_tests init_env
make -f Makefile.leak_tests full_leak_suite | tee leak_run.log

# üéñÔ∏è Tag & certify after passing
git tag -a v2.0-proof -m "All nine leak suites green $(date +%F)"
git push origin v2.0-proof
```

### üî•üíÄ‚ö° Validation Battery Coverage

| Test Category | Purpose | Pass Criteria |
|---------------|---------|---------------|
| **Over-fitting Detection** | Hidden GSM8K, HumanEval Private, Randomized Labels | 90-98% accuracy (not memorization) |
| **Placeholder Sweeps** | No stub functions or TODO markers | Zero placeholder patterns found |
| **Router Validation** | Real domain routing, not illusions | ‚â•90% precision & recall |
| **Evolution Integrity** | Genuine LoRA adapters, not stubs | Files ‚â•20MB, SHA verified |
| **Network Isolation** | No cloud fallbacks or hidden APIs | Zero external packets |
| **Metric Honesty** | No performance inflation | Prometheus parity validated |
| **Security Compliance** | Policy adherence, no bypasses | 100% safety compliance |
| **Stress Testing** | Concurrency & load handling | 100 req/s sustained |
| **Audit Trail** | Tamper-proof logs | SHA chain integrity |

**üéñÔ∏è When ALL tests pass:** Automatic `v2.0-proof` certification with downloadable audit logs proving genuine performance gains.

**üíÄ When ANY test fails:** Deployment blocked until issues fixed - no exceptions, no mercy.

# V11 Emotional Swarm - Production Ready AI System

üß†‚ö° **Sub-10ms Emotional Consensus | 570x Performance Improvement | Democratic AI Evolution**

## üöÄ Quick Start

```bash
# Clone and run
git clone https://github.com/luminainterface/swarmAI.git
cd swarmAI
pip install -r requirements.txt

# Start the emotional swarm
python serve.py

# Test the system
python warmup_v11_swarm.py
```

## üèóÔ∏è Docker Deployment

```bash
# Production deployment (5-container stack)
docker-compose up -d

# Health check
curl http://localhost:8000/health
```

## üß™ Testing

```bash
# Run comprehensive test suite
pytest tests/

# Test emotional consensus timing
pytest tests/test_emotional_swarm.py -v

# Test 570x performance improvement
pytest tests/test_emotional_roundtable.py -v
```

## üéØ Core Features

- **9 Emotional Agents**: Joy, Fear, Anger, Sadness, Disgust, Surprise, Trust, Anticipation, Determination
- **Sub-10ms Consensus**: Democratic decision-making in milliseconds
- **570x Performance**: Optimized from 77000ms ‚Üí 135ms
- **Production Ready**: Docker deployment with monitoring
- **Autonomous Evolution**: AI emotions vote on system improvements

## üï∑Ô∏èüèãÔ∏è‚Äç‚ôÇÔ∏è Crawler-Trainer Integration Pipeline

The system features an autonomous challenge discovery and training pipeline that continuously evolves the AI's capabilities:

### üîÑ **Integrated Workflow**

```mermaid
graph LR
    A[Auto Crawler] --> B[Challenge Discovery]
    B --> C[Quality Validation]
    C --> D[Auto Feeder]
    D --> E[Job Queue]
    E --> F[Trainer Worker]
    F --> G[LoRA Adapters]
    G --> H[Model Evolution]
    H --> A
```

### üï∑Ô∏è **Auto Crawler** (`scripts/auto_crawler.py`)

**Multi-Domain Challenge Discovery System**

```bash
# Discover challenges across all domains
python scripts/auto_crawler.py --count 10

# Focus on specific domain
python scripts/auto_crawler.py --domain math --count 5

# Adaptive difficulty targeting
python scripts/auto_crawler.py --min-difficulty 6 --max-difficulty 8
```

**Supported Domains:**
- **Code**: Algorithm challenges, data structures, optimization problems
- **Logic**: Puzzles, deduction problems, constraint satisfaction
- **Math**: Algebra, geometry, calculus, statistics, number theory
- **Creative**: Writing prompts, innovation tasks, metaphor creation
- **Science**: Physics, chemistry, biology problem-solving

**Quality Metrics:**
- Difficulty scoring (1-10 scale)
- Quality validation (0.7+ threshold)
- Domain-specific templates
- Adaptive challenge generation

### üçΩÔ∏è **Auto Feeder Daemon** (`scripts/auto_feeder_daemon.py`)

**Intelligent Challenge Processing & Queue Management**

```bash
# Check system status
python scripts/auto_feeder_daemon.py --status

# Force immediate feeding
python scripts/auto_feeder_daemon.py --feed-now

# Run as background daemon
python scripts/auto_feeder_daemon.py --daemon

# Process specific challenge file
python scripts/auto_feeder_daemon.py --feed-file datasets/challenges.jsonl
```

**Key Features:**
- **Queue Health Monitoring**: Maintains 5-50 jobs in queue
- **Priority Domains**: Logic and math challenges get priority
- **Batch Processing**: Configurable batch sizes (default: 10)
- **Adaptive Difficulty**: Adjusts training parameters based on challenge difficulty
- **Archive Management**: Processes and archives completed challenge files

### üèãÔ∏è‚Äç‚ôÇÔ∏è **Trainer Worker** (`trainer/trainer_worker.py`)

**LoRA Adapter Training System**

```bash
# Start training worker
python trainer/trainer_worker.py

# Process specific job
python trainer/trainer_worker.py --single-job jobs/queue/math_challenge_001.yaml

# Worker status
python trainer/trainer_worker.py --status
```

**Training Pipeline:**
1. **Job Discovery**: Monitors `jobs/queue/` for YAML training jobs
2. **Challenge Processing**: Converts challenges to domain-specific training prompts
3. **LoRA Creation**: Generates Low-Rank Adaptation layers for efficient fine-tuning
4. **Training Execution**: Runs optimized training loops with adaptive parameters
5. **Adapter Storage**: Saves trained adapters to `