cloud:
  budget_usd: 10.0
  enabled: true
  providers:
    mistral:
      priority: 1
      timeout_ms: 5000
    openai:
      priority: 2
      timeout_ms: 8000
logging:
  file: logs/autogen.log
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  level: INFO
memory:
  dimension: 384
  max_entries: 10000
  persist_every: 100
  provider: faiss
  enabled: true
  redis_channel: "scratchpad_v2"
  cache_size: 1000
  episodic_limit: 240  # tokens for episodic context
sandbox:
  provider: auto                    # auto-detect: docker > wsl > firejail
  enabled: true                     # Enable sandbox execution
  timeout_seconds: 10               # Maximum execution time (increased for Week 2)
  memory_limit_mb: 512              # Memory limit (increased for OS commands)
  cpu_limit: 2.0                    # CPU cores limit (increased for OS commands)
  
  # Docker-specific settings (preferred on all platforms)
  docker:
    image: agent-sandbox:latest     # Container image
    network: none                   # No network access
    remove_after: true              # Clean up containers
    volumes: []
    working_dir: /tmp/execution
    
  # WSL-specific settings (Windows fallback)
  wsl:
    distro: Ubuntu                  # WSL distribution
    user: sandbox                   # Execution user
    
  # Firejail-specific settings (Linux native)
  firejail:
    profile: sandbox/profile.conf
    private_tmp: true
    no_network: true

# Week 2 - OS Executor Configuration
os_executor:
  max_cpu_seconds: 10               # CPU time limit for OS commands
  max_fs_changes: 100               # Filesystem change limit
  audit_enabled: true               # Enable audit logging
  
  # Command allowlist categories
  allowed_categories:
    - file_operation
    - directory_operation  
    - system_info
    - process_management
    
  # Platform-specific settings
  windows:
    shell: powershell               # Use PowerShell on Windows
    safe_commands_only: true        # Extra safety on Windows
    
  linux:
    shell: bash                     # Use bash on Linux
    firejail_enabled: true          # Use firejail when available

# Week 1 Foundation - Model Configuration  
models:
  local_tinyllama:
    provider: exllama2              # Week 2-1: ExLlama V2 upgrade
    enabled: true
    path: /models/tinyllama/TinyLlama-1B-chat-q4_k_m.gguf
    device: cuda:0
    dtype: q4_k_m
    vram_mb: 785                    # ExLlama V2 efficiency
    max_context: 4096
    batch_size: 2
    cache_8bit: true

# Week 1 Foundation - Voting Configuration
voting:
  min_confidence: 0.75              # Confidence floor
  use_stub_filter: true             # Enhanced stub filtering
  
# Week 1 Foundation - Specialist Configuration  
specialists:
  math_specialist:
    batch_size: 2
    max_new_tokens: 160
    temperature: 0.05               # Low temp for deterministic math
    
  code_specialist:
    batch_size: 2
    max_new_tokens: 160
    temperature: 0.1                # Low temp for code
    
  logic_specialist:
    batch_size: 2
    max_new_tokens: 160
    temperature: 0.1                # Low temp for logic
    
  knowledge_specialist:
    batch_size: 2
    max_new_tokens: 160
    temperature: 0.2                # Higher temp for creativity

qdrant:
  collection: "lumina_mem_v2"
  dimension: 768
  hnsw_m: 32
  ef_construction: 256

gpu:
  # Preserve Phase 1 optimizations
  quantization: true
  attention_backend: "sdpa"
  mixed_precision: true
  max_batch_size: 4

router:
  dynamic_batch:
    max_tokens: 2048  # Conservative to preserve GPU gains
    timeout_ms: 100
  
  confidence_gates:
    to_synth: 0.45    # From Phase 1 tuning
    to_premium: 0.20

performance:
  target_tokens_per_sec: 25   # Adjusted for memory overhead
  target_gpu_util: 35         # Preserve Phase 1 achievement
  max_latency_ms: 1500
