cloud_models:
- backend: cloud
  name: gpt4o_mini
  path: api:gpt-4o-mini
  price_per_1k_input: 0.4
  price_per_1k_output: 1.6
  provider: openai
  quality: 82
  vram_mb: 0
- backend: cloud
  name: claude_3_haiku
  path: api:claude-3-haiku-20240307
  price_per_1k_input: 0.25
  price_per_1k_output: 1.25
  provider: anthropic
  quality: 78
  vram_mb: 0
- backend: cloud
  name: mistral_large
  path: api:mistral-large-latest
  price_per_1k_input: 8.0
  price_per_1k_output: 24.0
  provider: mistral
  quality: 88
  vram_mb: 0
cloud_providers:
- mistral_general
- openai_fallback
confidence_thresholds:
  code_specialist: 0.8
  knowledge_specialist: 0.7
  logic_specialist: 0.75
  math_specialist: 0.85
  mistral_general: 0.5
cpu_heads:
- backend: openvino
  dtype: openvino_fp32
  name: openvino_head
  vram_mb: 0
fallback_head: mistral_general
gpu_heads_large:
- backend: transformers
  dtype: q4_K
  name: mistral_7b_instruct
  vram_mb: 3500
- backend: vllm
  dtype: q4_K
  name: llama2_70b_chat
  vram_mb: 8000
- backend: vllm
  dtype: q4_K
  name: mixtral_8x7b
  vram_mb: 6500
gpu_heads_math_optimized:
- backend: transformers
  dtype: q4_K_M
  gpu_layers: 32
  hf_model: microsoft/phi-2
  kv_cache_mb: 512
  name: lightning_math
  optimization_target: latency
  priority: critical
  rope_scaling: yarn
  specialized_for: math
  vram_mb: 800
- backend: transformers
  dtype: q4_K_S
  hf_model: microsoft/phi-2
  kv_cache_mb: 128
  name: math_specialist_0.8b
  specialized_for: math
  vram_mb: 512
gpu_heads_medium:
- backend: transformers
  dtype: q4_K
  name: phi2_2.7b
  vram_mb: 1600
- backend: transformers
  dtype: q4_K
  name: codellama_0.7b
  vram_mb: 450
- backend: transformers
  dtype: q4_K
  name: math_specialist_0.8b
  vram_mb: 512
- backend: transformers
  dtype: q4_K
  name: openchat_3.5_0.4b
  vram_mb: 2400
gpu_heads_minimal:
- backend: transformers
  dtype: q4_K_S
  hf_model: gpt2
  name: safety_guard_local
  vram_mb: 200
- backend: transformers
  dtype: q4_K_S
  hf_model: distilgpt2
  name: router_classifier
  vram_mb: 150
gpu_heads_optimized:
- backend: transformers
  dtype: q4_K_M
  hf_model: microsoft/phi-2
  kv_cache_mb: 256
  name: phi2_2.7b_q4
  vram_mb: 1400
- backend: transformers
  dtype: q4_K_S
  hf_model: microsoft/phi-2
  kv_cache_mb: 128
  name: tinyllama_1b_q4
  vram_mb: 350
- backend: transformers
  dtype: q4_K_S
  gpu_layers: 32
  hf_model: microsoft/phi-2
  kv_cache_mb: 256
  name: math_specialist_q4
  priority: high
  rope_scaling: yarn
  specialized_for: math
  vram_mb: 600
- backend: transformers
  dtype: q4_K_S
  hf_model: microsoft/phi-1_5
  kv_cache_mb: 64
  name: mistral_0.5b_q4
  vram_mb: 200
gpu_heads_small:
- backend: transformers
  dtype: q4_K
  name: tinyllama_1b
  vram_mb: 640
- backend: transformers
  dtype: q4_K
  name: mistral_0.5b
  vram_mb: 320
- backend: transformers
  dtype: q4_K
  name: qwen2_0.5b
  vram_mb: 350
- backend: transformers
  dtype: q4_K
  name: safety_guard_0.3b
  vram_mb: 200
loading_strategy:
  cloud_ready:
    force_cloud_primary: true
    force_cpu: []
    priority_order:
    - gpu_heads_minimal
    - cloud_models
    vram_limit_mb: 2000
  gtx_1080:
    force_cpu: []
    priority_order:
    - gpu_heads_small
    - gpu_heads_medium
    vram_limit_mb: 8000
  quick_test:
    force_cpu: []
    priority_order:
    - gpu_heads_small
    vram_limit_mb: 1000
  rtx_4070:
    force_cpu: []
    lazy_load: true
    preload: ["tinyllama_1b", "safety_guard_0.3b"]  # Only Agent-0 + safety hot
    priority_order:
    - gpu_heads_small
    - gpu_heads_medium
    - gpu_heads_large
    vram_limit_mb: 10500
  rtx_4070_optimized:
    force_cpu: []
    max_kv_mb_total: 2048
    priority_order:
    - gpu_heads_optimized
    vram_limit_mb: 9500
# Week 2-1 Foundation - ExLlama V2 Local GPU Model (upgraded from transformers)
local_tinyllama:
  provider: exllama2
  enabled: true
  path: /models/tinyllama/TinyLlama-1B-chat-q4_k_m.gguf
  device: cuda:0
  dtype: q4_k_m
  vram_mb: 785              # ExLlama V2 more efficient than transformers
  max_context: 4096
  batch_size: 2
  cache_8bit: true

local_providers:
- math_specialist
- code_specialist
- logic_specialist
- knowledge_specialist
models:
- device: cuda:0
  dtype: q4_K
  name: llama2-70b-chat
  path: /models/llama2-70b-chat.gguf
- device: cuda:0
  dtype: q4_K
  name: mistral-7b-instruct
  path: /models/mistral-7b-instruct.gguf
- device: cuda:0
  dtype: q4_K
  name: mixtral-8x7b
  path: /models/mixtral-8x7b.gguf
profiles:
  working_test:
    enable_council: false
    max_concurrent: 1
    target_models:
    - mistral_0.5b
    - safety_guard_0.3b
    vram_limit_mb: 6000
specialists_order:
- math_specialist
- code_specialist
- logic_specialist
- knowledge_specialist
- mistral_general

# Local model pricing (cost per 1k tokens)
pricing:
  local_fp16: 0.0003          # Local GPU models
  math_specialist: 0.00005    # Specialized math head
  lightning_math: 0.00005     # Optimized math head  
  phi2_2.7b: 0.0001          # Medium local model
  tinyllama_1b: 0.00005      # Small local model
  mistral_0.5b: 0.00003      # Tiny local model
  mistral_medium_3: 0.0075   # Hybrid models
  gpt4o_mini: 0.01           # Cloud fallback

# ðŸš€ PERFORMANCE OPTIMIZATION #2: Hard caps to prevent runaway generation
specialist_defaults:
  max_tokens: 160             # Hard cap - was 128, now 160 to prevent truncation
  temperature: 0.0            # Greedy sampling for speed (no speculative decoding)
  timeout_seconds: 8          # Hard timeout - kill after 8s to prevent 37s delays
  do_sample: false           # Greedy sampling for deterministic fast responses
  generation_config:
    max_new_tokens: 160      # Hard cap at generation level
    do_sample: false
    temperature: 0.0
    pad_token_id: 0
    eos_token_id: 2
    use_cache: true          # Enable KV cache for speed
    rope_scaling: "yarn"     # Efficient rope scaling

# Specialist-specific overrides with hard caps
specialist_overrides:
  math_specialist:
    max_tokens: 64           # Math needs even fewer tokens for answers like "4"
    temperature: 0.0         # Deterministic math answers
    timeout_seconds: 8       # Hard timeout for math
    backend: transformers    # Use HuggingFace Transformers backend
  code_specialist:
    max_tokens: 160          # Code can be a bit longer but still capped
    temperature: 0.0         # Deterministic code generation
    timeout_seconds: 8       # Hard timeout for code
    backend: vllm            # Use vLLM backend for better code generation
  logic_specialist:
    max_tokens: 96           # Logic reasoning should be concise
    temperature: 0.0         # Deterministic logical conclusions
    timeout_seconds: 8       # Hard timeout for logic
    backend: transformers    # Use Transformers for logical reasoning
  knowledge_specialist:
    max_tokens: 160          # Knowledge answers capped at 160 tokens
    temperature: 0.1         # Slight creativity for knowledge explanations
    timeout_seconds: 8       # Hard timeout for knowledge
    backend: transformers    # Use Transformers for knowledge queries
  mistral_general:
    max_tokens: 20           # ðŸš€ WHISPER-SIZE: Agent-0 draft now just 16-24 tokens for bullet sketch
    temperature: 0.7         # More creative for general responses
    timeout_seconds: 5       # ðŸš€ FASTER: Reduced from 8s to 5s for quick drafts
    summary_tokens: 40       # ðŸš€ SUMMARY LIMIT: Truncate Agent-0 summaries to 40 tokens max
