embedder:
  model: bge-small-en-v1.5  # Faster 384-d embedder for memory integration
  device: cuda
  batch_size: 32
  max_length: 512
  normalize: true

# ðŸš€ FRONT-SPEAKER AGENT-0 Configuration
agent0:
  max_new_tokens: 24          # Draft size for immediate streaming
  confidence_gate: 0.60       # Threshold for skipping specialists
  temperature: 0.0            # Greedy for consistent responses
  timeout_seconds: 5          # Max time for Agent-0 draft
  
# Routing Configuration  
router:
  eager_stream: true          # Send Agent-0 draft immediately
  refine_async: true          # Run specialists in background
  background_timeout: 8       # Max time for specialist refinement

# Specialist Configuration
specialists:
  max_new_tokens: 160         # Keep existing hard limits
  timeout_seconds: 8          # 8s timeout for specialists
  sandbox_enabled: true       # Tool-heads run in sandbox only

# Backend Configuration for Phase 4 TensorRT-LLM
backends:
  # Current proven baseline (Phase 3 + Memory)
  transformers:
    enabled: true
    weight: 0.0  # Disabled for TensorRT testing
    max_tokens: 512
    temperature: 0.7
    
  # Phase 4: TensorRT-LLM Optimization
  tensorrt:
    enabled: true
    url: http://localhost:8081/v1
    weight: 1.0                # Primary backend for testing
    max_batch_tokens: 4096
    max_batch_size: 64
    draft_tokens: 0            # Speculative decoding handled by TRT
    timeout: 15                # 15s timeout for inference
    max_retries: 2
    
# Memory Integration (Phase 3)
memory:
  collection: lumina_mem_v3
  scratch_ttl: 3600            # 1 hour cache
  vector_limit: 5
  similarity_threshold: 0.7
  
# Performance Monitoring
monitoring:
  prometheus_port: 8090
  gpu_stats_interval: 1        # seconds
  
# Phase 4 Performance Targets
performance_targets:
  single_request:
    throughput_tps: 17         # tokens/sec
    latency_p95_ms: 800        # milliseconds
    gpu_utilization_min: 50    # percent
    
  batch_request:
    throughput_tps: 25         # tokens/sec at 4-way concurrency
    gpu_utilization_target: 55 # percent
    power_target_w: 75         # watts 